# TaskWorkerè¨­è¨ˆæ›¸

## æ¦‚è¦

`TaskWorker`ã¯ã€**ç‹¬ç«‹ã—ãŸãƒ—ãƒ­ã‚»ã‚¹**ã¨ã—ã¦å‹•ä½œã—ã€`TaskQueue`ã‚’ä»‹ã—ã¦ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’æ‹…å½“ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚`RedisCoordinator`ã®`dispatch_task`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é€šã˜ã¦ã‚¿ã‚¹ã‚¯ã‚’å—ä¿¡ã—ã€`TaskHandler`æŠ½è±¡åŒ–ã‚’é€šã˜ã¦æ§˜ã€…ãªå®Ÿè¡Œç’°å¢ƒï¼ˆãƒ—ãƒ­ã‚»ã‚¹å†…ç›´æ¥å®Ÿè¡Œã€Dockerå®Ÿè¡Œç­‰ï¼‰ã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚TaskWorkerãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ä¸»ã¨ãªã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œãƒ—ãƒ­ã‚»ã‚¹ã¨ã¯å®Œå…¨ã«åˆ†é›¢ã•ã‚Œã¦ãŠã‚Šã€Redisã‚’é€šã˜ãŸç–çµåˆãªé€šä¿¡ã§ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†æ•£å®Ÿè¡Œã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## è¨­è¨ˆç›®æ¨™

1. **ãƒ—ãƒ­ã‚»ã‚¹åˆ†é›¢**: TaskWorkerã¯å®Œå…¨ã«ç‹¬ç«‹ã—ãŸãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦å‹•ä½œ
2. **TaskQueueçµ±åˆ**: RedisTaskQueueç­‰ã®å…±æœ‰ã‚­ãƒ¥ãƒ¼ã‚’é€šã˜ãŸç–çµåˆãªé€£æº
3. **åˆ†æ•£å®Ÿè¡Œ**: è¤‡æ•°ã®TaskWorkerãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚‹æ°´å¹³åˆ†æ•£å‡¦ç†
4. **è€éšœå®³æ€§**: TaskWorkerãƒ—ãƒ­ã‚»ã‚¹ã®éšœå®³ãŒãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã«å½±éŸ¿ã—ãªã„è¨­è¨ˆ
5. **å®Ÿè¡Œç’°å¢ƒã®æŸ”è»Ÿæ€§**: InProcesså®Ÿè¡Œã¨Dockerå®Ÿè¡Œã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆ
6. **ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½çµ±åˆ**: TaskSpecã®ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã¨ã®é€£æº
7. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: è² è·ã«å¿œã˜ãŸTaskWorkerãƒ—ãƒ­ã‚»ã‚¹ã®å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ§‹æˆ

```mermaid
classDiagram
    class AbstractTaskQueue {
        <<abstract>>
        +enqueue(task_spec) bool
        +dequeue() TaskSpec
        +is_empty() bool
        +handle_task_failure(task_spec, error) bool
        +get_metrics() dict
        +configure(enable_retry, enable_metrics)
    }
    
    class InMemoryTaskQueue {
        -_queue: deque
        +enqueue(task_spec) bool
        +dequeue() TaskSpec
        +to_list() list[str]
    }
    
    class RedisTaskQueue {
        -redis: Redis
        -queue_key: str
        -specs_key: str
        +enqueue(task_spec) bool
        +dequeue() TaskSpec
        +to_list() list[str]
    }
    
    class TaskSpec {
        +node_id: str
        +execution_context: ExecutionContext
        +status: TaskStatus
        +retry_count: int
        +max_retries: int
        +last_error: str
        +can_retry() bool
        +increment_retry(error_message)
    }
    
    class TaskHandler {
        <<abstract>>
        +process_task(task) bool
        +_process_task(task) bool*
        +on_task_success(task, duration)
        +on_task_failure(task, error, duration)
        +on_task_timeout(task, duration)
    }
    
    class InProcessTaskExecutor {
        +_process_task(task) bool
    }
    
    class AsyncTaskExecutor {
        -concurrency: int
        -timeout: int
        +_process_task(task) bool
        +_process_task_async(task) bool
    }
    
    class DockerTaskExecutor {
        -docker_client: DockerClient
        -base_image: str
        +_process_task(task) bool
        +_create_execution_script(task) str
    }
    
    class TaskWorker {
        -queue: AbstractTaskQueue
        -handler: TaskHandler
        -worker_id: str
        -running: bool
        +start()
        +stop()
        -_worker_loop()
        -_execute_task(task_spec)
        +get_metrics() dict
    }
    
    AbstractTaskQueue <|-- InMemoryTaskQueue
    AbstractTaskQueue <|-- RedisTaskQueue
    AbstractTaskQueue --> TaskSpec
    TaskHandler <|-- InProcessTaskExecutor
    TaskHandler <|-- AsyncTaskExecutor
    TaskHandler <|-- DockerTaskExecutor
    TaskWorker --> AbstractTaskQueue
    TaskWorker --> TaskHandler
```

### TaskWorkerç‹¬ç«‹ãƒ—ãƒ­ã‚»ã‚¹ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
graph TD
    subgraph "Main Workflow Process"
        WE[WorkflowEngine]
        EC[ExecutionContext]
        WE --> EC
    end
    
    subgraph "Shared Storage (Redis)"
        RTQ[RedisTaskQueue]
        RTQ --> QueueData[(Queue Data)]
        RTQ --> SpecData[(TaskSpec Data)]
        RTQ --> ResultData[(Result Data)]
    end
    
    subgraph "TaskWorker Process 1"
        TW1[TaskWorker]
        TH1[TaskHandler]
        TW1 --> TH1
    end
    
    subgraph "TaskWorker Process 2"
        TW2[TaskWorker]
        TH2[TaskHandler]
        TW2 --> TH2
    end
    
    subgraph "TaskWorker Process N"
        TWN[TaskWorker]
        THN[TaskHandler]
        TWN --> THN
    end
    
    EC -.-> RTQ
    RTQ <--> TW1
    RTQ <--> TW2
    RTQ <--> TWN
    
    style WE fill:#e1f5fe
    style RTQ fill:#f3e5f5
    style TW1 fill:#e8f5e8
    style TW2 fill:#e8f5e8
    style TWN fill:#e8f5e8
```

### ç‹¬ç«‹ãƒ—ãƒ­ã‚»ã‚¹é€šä¿¡ãƒ•ãƒ­ãƒ¼

```mermaid
sequenceDiagram
    participant WE as WorkflowEngine
    participant RC as RedisCoordinator
    participant RTQ as RedisTaskQueue
    participant TW as TaskWorker Process
    participant TH as TaskHandler

    Note over WE,RC: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œãƒ—ãƒ­ã‚»ã‚¹
    WE->>RC: dispatch_task(TaskSpec, group_id)
    RC->>RTQ: LPUSH task_queue:{group_id}, task_data
    
    Note over TW,RTQ: TaskWorkerãƒ—ãƒ­ã‚»ã‚¹ï¼ˆç‹¬ç«‹ï¼‰
    loop TaskWorker Main Loop
        TW->>RTQ: dequeue() via Redis connection
        RTQ->>RTQ: RPOP task_queue:{group_id}
        RTQ-->>TW: TaskSpec (deserialized)
        
        alt TaskSpec available
            TW->>TW: Resolve Task from TaskSpec
            TW->>TH: process_task(task)
            
            alt Docker/Async Handler
                TH->>TH: Execute task in environment
                TH-->>TW: execution result
            end
            
            alt success
                TW->>RTQ: Store result via Redis
                TW->>RTQ: Update TaskSpec status = SUCCESS
            else failure
                TW->>RTQ: handle_task_failure()
                alt can_retry()
                    RTQ->>RTQ: Re-enqueue with updated retry_count
                else permanent failure
                    RTQ->>RTQ: Mark as ERROR
                end
            end
        else no task
            TW->>TW: sleep(polling_interval)
        end
    end
    
    Note over WE: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ã¯çµæœã‚’ç›£è¦–
    WE->>RTQ: Check task status and results
    RTQ-->>WE: Task completion status
```

## TaskHandlerã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¨­è¨ˆ

### åŸºåº•ã‚¯ãƒ©ã‚¹: TaskHandler

```python
class TaskHandler(ABC):
    """ã‚¿ã‚¹ã‚¯å‡¦ç†ã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def process_task(self, task: Task) -> bool:
        """ã‚¿ã‚¹ã‚¯å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
        
        æ©Ÿèƒ½:
        - é–¢æ•°ã‚¿ã‚¹ã‚¯ã®è‡ªå‹•è§£æ±º
        - åˆ†æ•£å®Ÿè¡Œã§ã®é–¢æ•°è§£æ±ºå¤±æ•—ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        - ã‚«ã‚¹ã‚¿ãƒ å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        """
        
    @abstractmethod
    def _process_task(self, task: Task) -> bool:
        """ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯å‡¦ç†ã®å®Ÿè£…ç‚¹"""
        
    def on_task_success(self, task: Task, duration: float) -> None:
        """ã‚¿ã‚¹ã‚¯æˆåŠŸæ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        
    def on_task_failure(self, task: Task, error: Exception, duration: float) -> None:
        """ã‚¿ã‚¹ã‚¯å¤±æ•—æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        
    def on_task_timeout(self, task: Task, duration: float) -> None:
        """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
```

### é–¢æ•°è§£æ±ºå‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯

```python
def process_task(self, task: Task) -> bool:
    if task.func is not None:
        # é–¢æ•°ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ç›´æ¥å®Ÿè¡Œ
        try:
            result = task()
            return True
        except Exception as e:
            return False
    else:
        # é–¢æ•°è§£æ±ºå¤±æ•—ã®æ¤œå‡ºã¨å‡¦ç†
        if task.payload.get("__is_function_task__"):
            func_meta = task.payload.get("__function_meta__")
            # æ˜ç¤ºçš„ã«å¤±æ•—ã•ã›ã‚‹
            return False
        # é€šå¸¸ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯å‡¦ç†
        return self._process_task(task)
```

## TaskHandlerå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

### å®Ÿè¡Œç’°å¢ƒåˆ¥å®Ÿè£…æˆ¦ç•¥

TaskHandlerã¯å®Ÿè¡Œç’°å¢ƒã«å¿œã˜ã¦ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ï¼š

1. **InProcesså®Ÿè¡Œ** - TaskWorkerãƒ—ãƒ­ã‚»ã‚¹å†…ã§ã®ç›´æ¥å®Ÿè¡Œ
2. **AsyncTaskExecutor** - éåŒæœŸä¸¦è¡Œå®Ÿè¡Œã«ã‚ˆã‚‹é«˜åŠ¹ç‡å‡¦ç†
3. **Dockerå®Ÿè¡Œ** - Dockerã‚³ãƒ³ãƒ†ãƒŠã§ã®åˆ†é›¢å®Ÿè¡Œ

### 1. InProcessTaskExecutor (ãƒ—ãƒ­ã‚»ã‚¹å†…ç›´æ¥å®Ÿè¡Œ)

```python
class InProcessTaskExecutor(TaskHandler):
    """TaskWorkerãƒ—ãƒ­ã‚»ã‚¹å†…ã§ã®ç›´æ¥å®Ÿè¡Œã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿ãƒ¼"""
    
    def _process_task(self, task: Task) -> bool:
        """ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ã‚¿ã‚¹ã‚¯ã‚’ç›´æ¥å®Ÿè¡Œ"""
        try:
            # åŒä¸€ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ç›´æ¥å®Ÿè¡Œ
            result = task()
            return True
        except Exception as e:
            logger.error(f"InProcess task execution failed: {e}")
            return False
```

### 2. AsyncTaskExecutor (éåŒæœŸä¸¦è¡Œå®Ÿè¡Œ)

```python
class AsyncTaskExecutor(TaskHandler):
    """éåŒæœŸå‡¦ç†ã‚’è¡Œã†TaskExecutor"""
    
    def __init__(self, concurrency: int = 10, timeout: int = 300):
        super().__init__()
        self.concurrency = concurrency
        self.timeout = timeout
        
        # éåŒæœŸå®Ÿè¡Œç”¨ã®è¨­å®š
        self._loop: Optional[asyncio.AbstractEventLoop] = None
        self._loop_thread: Optional[threading.Thread] = None
        self._setup_lock = threading.Lock()
        self._semaphore: Optional[asyncio.Semaphore] = None
    
    def _process_task(self, task: Task) -> bool:
        """TaskHandlerã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®å®Ÿè£…"""
        if self._loop is None:
            self._setup_async_loop()
        
        try:
            future = asyncio.run_coroutine_threadsafe(
                self._process_task_async(task),
                self._loop
            )
            return future.result()
        except Exception as e:
            logger.error(f"AsyncTaskExecutor failed: {e}")
            return False
    
    def _setup_async_loop(self):
        """éåŒæœŸãƒ«ãƒ¼ãƒ—ã‚’å°‚ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰ã§è¨­å®š"""
        with self._setup_lock:
            if self._loop is not None:
                return
            
            loop_ready = threading.Event()
            
            def run_loop():
                try:
                    self._loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(self._loop)
                    self._semaphore = asyncio.Semaphore(self.concurrency)
                    loop_ready.set()
                    self._loop.run_forever()
                except Exception as e:
                    logger.error(f"Event loop error: {e}")
                finally:
                    if self._loop:
                        self._loop.close()
            
            self._loop_thread = threading.Thread(target=run_loop, daemon=True)
            self._loop_thread.start()
            
            loop_ready.wait(timeout=5.0)
            if not loop_ready.is_set():
                raise RuntimeError("Failed to start async event loop")
    
    async def _process_task_async(self, task: Task) -> bool:
        """éåŒæœŸã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ"""
        async with self._semaphore:
            try:
                loop = asyncio.get_event_loop()
                result = await asyncio.wait_for(
                    loop.run_in_executor(None, task),
                    timeout=self.timeout
                )
                return True
            except asyncio.TimeoutError:
                logger.warning(f"Task {task.task_id} timed out after {self.timeout}s")
                return False
            except Exception as e:
                logger.error(f"Async task execution failed: {e}")
                return False
    
    def shutdown(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self._loop and not self._loop.is_closed():
            self._loop.call_soon_threadsafe(self._loop.stop)
        
        if self._loop_thread and self._loop_thread.is_alive():
            self._loop_thread.join(timeout=5.0)
```

### 3. DockerTaskExecutor (Dockerå®Ÿè¡Œ)

```python
class DockerTaskExecutor(TaskHandler):
    """Dockerã‚³ãƒ³ãƒ†ãƒŠã§ã®ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self, 
                 base_image: str = "python:3.10-slim",
                 docker_client=None,
                 volume_mounts: Dict[str, str] = None,
                 environment: Dict[str, str] = None,
                 timeout: int = 300):
        """
        Args:
            base_image: ãƒ™ãƒ¼ã‚¹Dockerã‚¤ãƒ¡ãƒ¼ã‚¸
            docker_client: Dockerã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ (Noneæ™‚ã¯è‡ªå‹•ä½œæˆ)
            volume_mounts: ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒã‚¦ãƒ³ãƒˆè¨­å®š
            environment: ç’°å¢ƒå¤‰æ•°è¨­å®š
            timeout: ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (ç§’)
        """
        self.base_image = base_image
        self.volume_mounts = volume_mounts or {}
        self.environment = environment or {}
        self.timeout = timeout
        
        if docker_client is None:
            import docker
            self.docker_client = docker.from_env()
        else:
            self.docker_client = docker_client
    
    def _process_task(self, task: Task) -> bool:
        """Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ"""
        try:
            # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆ
            execution_script = self._create_execution_script(task)
            
            # ã‚³ãƒ³ãƒ†ãƒŠè¨­å®š
            container_config = {
                "image": self.base_image,
                "command": ["python", "-c", execution_script],
                "environment": self.environment,
                "volumes": self.volume_mounts,
                "detach": True,
                "remove": True,  # å®Ÿè¡Œå¾Œè‡ªå‹•å‰Šé™¤
                "network_mode": "bridge"
            }
            
            # ã‚³ãƒ³ãƒ†ãƒŠå®Ÿè¡Œ
            container = self.docker_client.containers.run(**container_config)
            
            # å®Ÿè¡Œå®Œäº†å¾…ã¡
            result = container.wait(timeout=self.timeout)
            
            # ãƒ­ã‚°å–å¾—
            logs = container.logs().decode('utf-8')
            
            # å®Ÿè¡Œçµæœåˆ¤å®š
            exit_code = result['StatusCode']
            success = exit_code == 0
            
            if success:
                logger.info(f"Docker task completed successfully")
                logger.debug(f"Container logs: {logs}")
            else:
                logger.error(f"Docker task failed with exit code {exit_code}")
                logger.error(f"Container logs: {logs}")
            
            return success
            
        except Exception as e:
            logger.error(f"Docker task execution failed: {e}")
            return False
    
    def _create_execution_script(self, task: Task) -> str:
        """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œç”¨Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        import pickle
        import base64
        
        # ã‚¿ã‚¹ã‚¯ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã—ã¦base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        task_bytes = pickle.dumps(task)
        task_b64 = base64.b64encode(task_bytes).decode('utf-8')
        
        script = f'''
import pickle
import base64
import sys
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    # ã‚¿ã‚¹ã‚¯ã‚’ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
    task_data = base64.b64decode("{task_b64}")
    task = pickle.loads(task_data)
    
    # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
    logger.info(f"Executing task: {{task.task_id}}")
    result = task()
    
    logger.info(f"Task completed successfully: {{result}}")
    print("DOCKER_TASK_SUCCESS")
    sys.exit(0)
    
except Exception as e:
    logger.error(f"Task execution failed: {{e}}")
    print(f"DOCKER_TASK_ERROR: {{e}}")
    sys.exit(1)
'''
        return script
    
    def on_task_failure(self, task: Task, error: Exception, duration: float) -> None:
        """Dockerå®Ÿè¡Œå¤±æ•—æ™‚ã®è©³ç´°ãƒ­ã‚°"""
        logger.error(f"Docker task {task.task_id} failed after {duration:.3f}s: {error}")
        logger.error(f"Check Docker daemon status and image availability: {self.base_image}")
```


## TaskWorkerè¨­è¨ˆ

### ä¸»è¦æ©Ÿèƒ½

1. **ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ã¨ã®é€£æº**
   - `AbstractTaskQueue`ã‹ã‚‰ã®ã‚¿ã‚¹ã‚¯å–å¾—
   - ã‚¿ã‚¹ã‚¯å®Œäº†ãƒ»å¤±æ•—ã®çŠ¶æ…‹æ›´æ–°
   - ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã¨ã®çµ±åˆ

2. **å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†**
   - `ExecutionContext`ã¨ã®é€£æº
   - ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è§£æ±º
   - çŠ¶æ…‹ç®¡ç†ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°

3. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**
   - ä¾‹å¤–ã‚­ãƒ£ãƒƒãƒã¨åˆ†é¡
   - ãƒªãƒˆãƒ©ã‚¤å¯èƒ½æ€§ã®åˆ¤å®š
   - æ°¸ç¶šçš„å¤±æ•—ã®å‡¦ç†

4. **ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãƒ­ã‚°**
   - å®Ÿè¡Œæ™‚é–“ã®è¨ˆæ¸¬
   - æˆåŠŸç‡ã®è¿½è·¡
   - è©³ç´°ãªãƒ­ã‚°å‡ºåŠ›

### ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†

```python
class TaskWorker:
    def start(self) -> None:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã§é–‹å§‹"""
        
    def stop(self) -> None:
        """ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        
    def _worker_loop(self) -> None:
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—å‡¦ç†"""
        while self.running:
            task_spec = self.queue.dequeue()
            if task_spec:
                self._execute_task(task_spec)
            else:
                time.sleep(0.1)  # ãƒãƒ¼ãƒªãƒ³ã‚°é–“éš”
```

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ©Ÿèƒ½

```python
def get_metrics(self) -> dict:
    return {
        "worker_id": self.worker_id,
        "tasks_processed": self.tasks_processed,
        "tasks_succeeded": self.tasks_succeeded,
        "tasks_failed": self.tasks_failed,
        "total_execution_time": self.total_execution_time,
        "average_execution_time": self.total_execution_time / self.tasks_processed,
        "success_rate": self.tasks_succeeded / self.tasks_processed
    }
```

## TaskWorkerç‹¬ç«‹ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

### 1. åŸºæœ¬çš„ãªç‹¬ç«‹ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ

```python
# TaskWorkerç‹¬ç«‹ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# worker_main.py
import os
import sys
import time
import signal
from graflow.worker import TaskWorker, InProcessTaskExecutor, DockerTaskExecutor, AsyncTaskExecutor
from graflow.queue.redis import RedisTaskQueue

def create_redis_queue(redis_config: dict) -> RedisTaskQueue:
    """Redis TaskQueueæ¥ç¶šã‚’ä½œæˆ"""
    import redis
    redis_client = redis.Redis(
        host=redis_config['host'],
        port=redis_config['port'],
        db=redis_config.get('db', 0),
        decode_responses=True
    )
    
    # ãƒ€ãƒŸãƒ¼ã®ExecutionContext (TaskQueueã®ã¿å¿…è¦)
    dummy_context = type('DummyContext', (), {
        'session_id': redis_config.get('session_id', 'default_session')
    })()
    
    return RedisTaskQueue(
        execution_context=dummy_context,
        redis_client=redis_client,
        key_prefix=redis_config.get('key_prefix', 'graflow')
    )

def create_task_handler(handler_config: dict):
    """è¨­å®šã«åŸºã¥ã„ã¦TaskHandlerä½œæˆ"""
    handler_type = handler_config.get('type', 'inprocess')
    
    if handler_type == 'docker':
        return DockerTaskExecutor(
            base_image=handler_config.get('base_image', 'python:3.10'),
            timeout=handler_config.get('timeout', 1800),
            environment=handler_config.get('environment', {}),
            volume_mounts=handler_config.get('volume_mounts', {})
        )
    elif handler_type == 'async':
        return AsyncTaskExecutor(
            concurrency=handler_config.get('concurrency', 10),
            timeout=handler_config.get('timeout', 300)
        )
    else:  # inprocess
        return InProcessTaskExecutor()

def main():
    """TaskWorkerã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šå–å¾—
    worker_id = os.environ.get('WORKER_ID', f'worker_{os.getpid()}')
    
    redis_config = {
        'host': os.environ.get('REDIS_HOST', 'localhost'),
        'port': int(os.environ.get('REDIS_PORT', '6379')),
        'db': int(os.environ.get('REDIS_DB', '0')),
        'key_prefix': os.environ.get('REDIS_KEY_PREFIX', 'graflow'),
        'session_id': os.environ.get('SESSION_ID', 'default_session')
    }
    
    handler_config = {
        'type': os.environ.get('HANDLER_TYPE', 'inprocess'),
        'base_image': os.environ.get('DOCKER_IMAGE', 'python:3.10'),
        'timeout': int(os.environ.get('TASK_TIMEOUT', '1800'))
    }
    
    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
    worker = None
    
    def signal_handler(signum, frame):
        print(f"Worker {worker_id} received signal {signum}, shutting down...")
        if worker:
            worker.stop()
        sys.exit(0)
    
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        # Redisæ¥ç¶šã¨TaskHandleræº–å‚™
        redis_queue = create_redis_queue(redis_config)
        task_handler = create_task_handler(handler_config)
        
        # TaskWorkerä½œæˆ
        worker = TaskWorker(
            queue=redis_queue,
            handler=task_handler,
            worker_id=worker_id
        )
        
        print(f"TaskWorker {worker_id} starting...")
        
        # TaskWorkeré–‹å§‹ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°å®Ÿè¡Œï¼‰
        worker.start()
        
    except Exception as e:
        print(f"TaskWorker {worker_id} failed to start: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```


### 2. TaskSpecæ´»ç”¨ + ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½çµ±åˆ

```python
# TaskSpecã®ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã¨ã®çµ±åˆï¼ˆPhase 3å¯¾å¿œï¼‰
from graflow.worker import TaskWorker, InProcessTaskExecutor
from graflow.queue.memory import InMemoryTaskQueue

# ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½æœ‰åŠ¹åŒ–
context = ExecutionContext(
    graph,
    start_node="task1",
    queue_backend="in_memory",
    queue_config={
        'enable_retry': True,      # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½æœ‰åŠ¹åŒ–
        'enable_metrics': True     # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ©Ÿèƒ½æœ‰åŠ¹åŒ–
    }
)

# ã‚­ãƒ¥ãƒ¼ã®ãƒªãƒˆãƒ©ã‚¤è¨­å®š
context.queue.configure(enable_retry=True, enable_metrics=True)

# TaskWorker with retry integration
class RetryAwareTaskHandler(AsyncTaskExecutor):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã‚’èªè­˜ã™ã‚‹TaskHandler"""
    
    def on_task_failure(self, task: Task, error: Exception, duration: float) -> None:
        # ãƒªãƒˆãƒ©ã‚¤æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.error(f"Task {task.task_id} failed after {duration:.3f}s: {error}")
        logger.info(f"Task will be retried (retry_count will be incremented)")
        
    def on_task_success(self, task: Task, duration: float) -> None:
        logger.info(f"Task {task.task_id} succeeded after {duration:.3f}s")

# ãƒªãƒˆãƒ©ã‚¤å¯¾å¿œãƒ¯ãƒ¼ã‚«ãƒ¼
retry_handler = RetryAwareTaskHandler()
worker = TaskWorker(
    queue=context.queue,
    handler=retry_handler,
    worker_id="retry_worker"
)

worker.start()

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ä¾‹
import time
time.sleep(10)  # ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè¡Œå¾…ã¡

queue_metrics = context.queue.get_metrics()
worker_metrics = worker.get_metrics()

print("Queue Metrics:", queue_metrics)
print("Worker Metrics:", worker_metrics)
```

### 3. è¨­å®šé§†å‹•å‹å®Ÿè¡Œãƒ‘ã‚¿ãƒ¼ãƒ³

```python
# YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (worker_config.yaml)
"""
workers:
  - id: "fast_worker"
    handler_type: "inprocess"
      
  - id: "async_worker" 
    handler_type: "async"
    config:
      concurrency: 20
      timeout: 300
      
  - id: "docker_worker"
    handler_type: "docker"
    config:
      base_image: "python:3.10-slim"
      timeout: 300
      environment:
        PYTHONPATH: "/workspace"
        
  - id: "ml_worker"
    handler_type: "docker"
    config:
      base_image: "tensorflow/tensorflow:latest"
      timeout: 3600
      environment:
        CUDA_VISIBLE_DEVICES: "0"
      volume_mounts:
        "/host/models": 
          bind: "/workspace/models"
          mode: "rw"
"""

import yaml

def create_worker_from_config(config_file: str, queue, context):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½œæˆ"""
    with open(config_file) as f:
        config = yaml.safe_load(f)
    
    workers = []
    for worker_config in config['workers']:
        handler_type = worker_config['handler_type']
        handler_config = worker_config.get('config', {})
        
        if handler_type == "inprocess":
            handler = InProcessTaskExecutor()
        elif handler_type == "async":
            handler = AsyncTaskExecutor(
                concurrency=handler_config.get('concurrency', 10),
                timeout=handler_config.get('timeout', 300)
            )
        elif handler_type == "docker":
            handler = DockerTaskExecutor(
                base_image=handler_config.get('base_image', 'python:3.10'),
                timeout=handler_config.get('timeout', 300),
                environment=handler_config.get('environment', {}),
                volume_mounts=handler_config.get('volume_mounts', {})
            )
        else:
            raise ValueError(f"Unknown handler type: {handler_type}")
        
        worker = TaskWorker(
            queue=queue,
            handler=handler,
            worker_id=worker_config['id']
        )
        workers.append(worker)
    
    return workers

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¯ãƒ¼ã‚«ãƒ¼ä½œæˆãƒ»èµ·å‹•
workers = create_worker_from_config("worker_config.yaml", queue, context)
for worker in workers:
    worker.start()
```

### 4. RedisCoordinatorã¨ã®çµ±åˆ

TaskWorkerã¯ã€RedisCoordinatorã®`dispatch_task`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é€šã˜ã¦ã‚¿ã‚¹ã‚¯ã‚’å—ä¿¡ã—ã€ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†æ•£å®Ÿè¡Œã‚’å®Ÿç¾ã—ã¾ã™ã€‚

#### åŸºæœ¬çš„ãªçµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³

```python
from graflow.coordination.redis import RedisCoordinator
from graflow.worker import TaskWorker, AsyncTaskExecutor
from graflow.queue.redis import RedisTaskQueue

# RedisCoordinatorè¨­å®š
redis_config = {
    'host': 'localhost',
    'port': 6379,
    'db': 0
}

coordinator = RedisCoordinator(redis_client)

# TaskWorkerã®ä½œæˆ
def create_worker(worker_id: str, group_id: str):
    """TaskWorkerä½œæˆ"""
    
    # TaskQueueã‚’ä½œæˆï¼ˆRedisCoordinatorã¨åŒã˜Redisä½¿ç”¨ï¼‰
    dummy_context = type('DummyContext', (), {
        'session_id': f"session_{group_id}"
    })()
    
    task_queue = RedisTaskQueue(
        execution_context=dummy_context,
        redis_client=redis_client,
        key_prefix=f"workflow_{group_id}"
    )
    
    # TaskHandlerã‚’é¸æŠ
    handler = AsyncTaskExecutor(concurrency=10, timeout=300)
    
    # TaskWorkerä½œæˆ
    worker = TaskWorker(
        queue=task_queue,
        handler=handler,
        worker_id=worker_id
    )
    
    return worker

# ä½¿ç”¨ä¾‹ï¼šdispatch_task â†’ TaskWorkeré€£æº
group_id = 'processing_group'

# è¤‡æ•°ã®Workerã‚’ä½œæˆ
workers = []
for i in range(3):
    worker_id = f"worker_{i}"
    worker = create_worker(worker_id, group_id)
    workers.append(worker)
    worker.start()  # éåŒæœŸã§é–‹å§‹

# RedisCoordinatorã‹ã‚‰ã‚¿ã‚¹ã‚¯ã‚’dispatch
# coordinator.dispatch_task()ãŒå‘¼ã°ã‚Œã‚‹ã¨ã€TaskQueueã«è‡ªå‹•çš„ã«enqueueã•ã‚Œã€
# å¾…æ©Ÿä¸­ã®TaskWorkerãŒè‡ªå‹•çš„ã«ã‚¿ã‚¹ã‚¯ã‚’å–å¾—ãƒ»å®Ÿè¡Œ
```

#### dispatch_taskçµ±åˆã®ä»•çµ„ã¿

```python
# RedisCoordinator.dispatch_task()ã®å‹•ä½œ
def dispatch_task(self, task_spec: TaskSpec, group_id: str) -> None:
    """Dispatch task to Redis queue for worker processing."""
    queue_key = f"task_queue:{group_id}"
    
    # ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã—ã¦Redisã‚­ãƒ¥ãƒ¼ã«æŠ•å…¥
    task_data = {
        "task_id": task_spec.task_id,
        "func_name": getattr(task_spec.func, '__name__', str(task_spec.func)),
        "args": task_spec.args if hasattr(task_spec, 'args') else (),
        "kwargs": task_spec.kwargs if hasattr(task_spec, 'kwargs') else {},
        "group_id": group_id,
        "timestamp": time.time()
    }
    
    # Redisã‚­ãƒ¥ãƒ¼ã«å·¦ã‹ã‚‰ãƒ—ãƒƒã‚·ãƒ¥ï¼ˆFIFOã§å³ã‹ã‚‰å–å¾—ï¼‰
    self.redis.lpush(queue_key, json.dumps(task_data))

# TaskWorkerãŒåŒã˜ã‚­ãƒ¥ãƒ¼ã‚’ç›£è¦–ã—ã¦è‡ªå‹•å®Ÿè¡Œ
# TaskWorker._worker_loop()ã§dequeue()ã‚’å‘¼ã³å‡ºã—ã€
# RedisTaskQueue.dequeue()ãŒredis.rpop()ã§ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
```


## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è€ƒæ…®äº‹é …

### 1. ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£

- `threading.Lock`ã‚’ä½¿ç”¨ã—ãŸçŠ¶æ…‹ç®¡ç†
- ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªæ“ä½œã§ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
- ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯å›é¿ã®ãŸã‚ã®é©åˆ‡ãªãƒ­ãƒƒã‚¯é †åº

### 2. ãƒ¡ãƒ¢ãƒªç®¡ç†

- ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®é©åˆ‡ãªè§£æ”¾
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å®šæœŸçš„ãªãƒªã‚»ãƒƒãƒˆ
- é•·æ™‚é–“å®Ÿè¡Œã§ã®ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯é˜²æ­¢

### 3. CPUåŠ¹ç‡

- ãƒãƒ¼ãƒªãƒ³ã‚°é–“éš”ã®æœ€é©åŒ–
- ä¸è¦ãªé–¢æ•°å‘¼ã³å‡ºã—ã®å‰Šæ¸›
- åŠ¹ç‡çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

## æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ

### 1. ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæˆ¦ç•¥ã®æ‹¡å¼µ

- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ã®å®Ÿè£…
- å„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ã®å®Ÿè¡Œé †åº
- ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ã®é©ç”¨

### 2. ç›£è¦–æ©Ÿèƒ½ã®å¼·åŒ–

- ãƒ—ãƒ­ãƒ¡ãƒ†ã‚¦ã‚¹ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‡ºåŠ›
- åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°çµ±åˆ
- ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã®å®Ÿè£…

## ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### 1. å˜ä½“ãƒ†ã‚¹ãƒˆ

- TaskHandlerã®å„å®Ÿè£…ã®ãƒ†ã‚¹ãƒˆ
- TaskWorkerã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒ†ã‚¹ãƒˆ
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ

### 2. çµ±åˆãƒ†ã‚¹ãƒˆ

- TaskQueueã¨ã®é€£æºãƒ†ã‚¹ãƒˆ
- ExecutionContextã¨ã®çµ±åˆãƒ†ã‚¹ãƒˆ
- å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã®å‹•ä½œç¢ºèª

### 3. è² è·ãƒ†ã‚¹ãƒˆ

- å¤§é‡ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½æ¸¬å®š
- é•·æ™‚é–“å®Ÿè¡Œã§ã®å®‰å®šæ€§ç¢ºèª
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–

## ç¾åœ¨ã®å®Ÿè£…çŠ¶æ³ï¼ˆ2025å¹´8æœˆ12æ—¥æ™‚ç‚¹ï¼‰

### âœ… å®Ÿè£…å®Œäº† (Phase 1åŸºæœ¬éƒ¨åˆ†)
- [x] TaskHandleræŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹è¨­è¨ˆãƒ»å®Ÿè£…
- [x] InProcessTaskExecutorå®Ÿè£…
- [x] TaskWorkeråŸºæœ¬å®Ÿè£…ï¼ˆä¸¦è¡Œå®Ÿè¡Œãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¯¾å¿œï¼‰
- [x] ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (main.py)
- [x] åŸºæœ¬çš„ãªãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
- [x] InMemoryTaskQueueçµ±åˆãƒ†ã‚¹ãƒˆï¼ˆåŸºæœ¬å‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
- [x] RedisTaskQueueçµ±åˆå¯¾å¿œï¼ˆæ¥ç¶šãƒ»åŸºæœ¬æ©Ÿèƒ½å®Ÿè£…æ¸ˆã¿ï¼‰

### ğŸ“ å®Ÿè£…ã®ç‰¹å¾´
- **ä¸¦è¡Œå®Ÿè¡Œ**: ThreadPoolExecutorã«ã‚ˆã‚‹ä¸¦è¡Œã‚¿ã‚¹ã‚¯å‡¦ç†
- **ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³**: ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ã‚¹ã‚¯å®Œäº†å¾…ã¡
- **ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: æˆåŠŸç‡ãƒ»å®Ÿè¡Œæ™‚é–“ãƒ»å‡¦ç†æ•°ã®è©³ç´°è¿½è·¡
- **è¨­å®šé§†å‹•**: ç’°å¢ƒå¤‰æ•°ãƒ»ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã«ã‚ˆã‚‹æŸ”è»Ÿãªè¨­å®š
- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒ»ä¾‹å¤–å‡¦ç†ãƒ»ãƒ­ã‚°çµ±åˆ

## æ®µéšçš„å®Ÿè£…è¨ˆç”»

### Phase 1.1: é«˜åº¦ãªTaskHandlerå®Ÿè£… (NEXT)
- [ ] AsyncTaskExecutorå®Ÿè£…
  - [ ] éåŒæœŸå®Ÿè¡Œãƒ«ãƒ¼ãƒ—è¨­å®š
  - [ ] ã‚»ãƒãƒ•ã‚©ã«ã‚ˆã‚‹ä¸¦è¡Œåˆ¶å¾¡
  - [ ] ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†
- [ ] DockerTaskExecutorå®Ÿè£…
  - [ ] Dockerã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆçµ±åˆ
  - [ ] ã‚³ãƒ³ãƒ†ãƒŠå®Ÿè¡Œãƒ»ç›£è¦–
  - [ ] ã‚¿ã‚¹ã‚¯ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
- [ ] main.pyã§ã®æ–°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å¯¾å¿œ
- [ ] å„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å˜ä½“ãƒ†ã‚¹ãƒˆ

### Phase 1.2: ExecutionContextçµ±åˆ (HIGH PRIORITY)
- [ ] TaskSpec â†’ å®ŸTaskã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè§£æ±ºæ©Ÿèƒ½
  - [ ] ExecutionContextã‹ã‚‰ã®ãƒãƒ¼ãƒ‰è§£æ±º
  - [ ] ã‚°ãƒ©ãƒ•ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«çµ±åˆ
  - [ ] ã‚¿ã‚¹ã‚¯é–¢æ•°ã®å‹•çš„ãƒ­ãƒ¼ãƒ‰
- [ ] MockTaskå®Ÿè£…ã‹ã‚‰å®Ÿéš›ã®Taskçµ±åˆã¸ã®ç§»è¡Œ
- [ ] é–¢æ•°è§£æ±ºå¤±æ•—ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆåˆ†æ•£å®Ÿè¡Œå¯¾å¿œï¼‰
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆå®Ÿãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã®å‹•ä½œç¢ºèªï¼‰

### Phase 2: åˆ†æ•£å®Ÿè¡Œå¼·åŒ–
- [ ] RedisCoordinatorã¨ã®å®Œå…¨çµ±åˆ
  - [ ] dispatch_task â†’ TaskWorkerè‡ªå‹•é€£æº
  - [ ] è¤‡æ•°Workerãƒ—ãƒ­ã‚»ã‚¹ã§ã®LoadBalancing
  - [ ] ã‚¿ã‚¹ã‚¯çµæœã®RedisçµŒç”±åŒæœŸ
- [ ] TaskSpecãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½çµ±åˆ
  - [ ] handle_task_failureã¨ã®é€£æº
  - [ ] æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ãƒ»ãƒªãƒˆãƒ©ã‚¤åˆ¶å¾¡
- [ ] åˆ†æ•£å®Ÿè¡Œç›£è¦–ãƒ»ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½

### Phase 3: æœ¬æ ¼é‹ç”¨å¯¾å¿œ
- [ ] é«˜åº¦ãªå®Ÿè¡Œç’°å¢ƒå¯¾å¿œ
  - [ ] Dockerå®Ÿè¡Œã§ã®ç’°å¢ƒåˆ†é›¢
  - [ ] ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
  - [ ] ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ»ãƒœãƒªãƒ¥ãƒ¼ãƒ å¯¾å¿œ
- [ ] é‹ç”¨ãƒ»ç›£è¦–æ©Ÿèƒ½
  - [ ] ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ»ç”Ÿå­˜ç›£è¦–
  - [ ] ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆPrometheusç­‰ï¼‰
  - [ ] å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŸºç›¤
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### Phase 4: ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºæ©Ÿèƒ½
- [ ] é«˜å¯ç”¨æ€§ãƒ»è€éšœå®³æ€§
  - [ ] Workerãƒ—ãƒ­ã‚»ã‚¹è‡ªå‹•å¾©æ—§
  - [ ] ã‚¿ã‚¹ã‚¯å®Ÿè¡Œä¿è¨¼æ©Ÿèƒ½
  - [ ] Dead Letter Queue
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹
  - [ ] èªè¨¼ãƒ»èªå¯çµ±åˆ
  - [ ] ç›£æŸ»ãƒ­ã‚°ãƒ»ãƒˆãƒ¬ãƒ¼ã‚µãƒ“ãƒªãƒ†ã‚£
- [ ] é‹ç”¨è‡ªå‹•åŒ–ãƒ»DevOpsçµ±åˆ

## æ¬¡ã®å®Ÿè£…æ¨å¥¨é †åº

1. **Phase 1.1** (AsyncTaskExecutor, DockerTaskExecutor)
2. **Phase 1.2** (ExecutionContextçµ±åˆ) - æœ€ã‚‚é‡è¦
3. **Phase 2** (åˆ†æ•£å®Ÿè¡Œãƒ»Redisçµ±åˆ)
4. **Phase 3** (é‹ç”¨ãƒ»ç›£è¦–)

ã“ã®æ®µéšçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€å®Ÿç”¨çš„ã§å …ç‰¢ãªTaskWorkerã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚