# Graflow ã‚³ãƒ¼ãƒ‰æ”¹å–„ææ¡ˆï¼ˆçµ±åˆç‰ˆï¼‰

**æœ€çµ‚æ›´æ–°æ—¥:** 2025-12-08
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³:** 3.0ï¼ˆçµ±åˆç‰ˆï¼‰
**å¯¾è±¡ãƒªãƒªãƒ¼ã‚¹:** v0.3.0+

## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

æœ¬æ–‡æ›¸ã¯ã€Claudeã€Codexã€Geminiã®3ã¤ã®ç‹¬ç«‹ã—ãŸã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’çµ±åˆã—ã€Graflowã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ”¹å–„ææ¡ˆã‚’å„ªå…ˆåº¦é †ã«ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯HITLã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã€åˆ†æ•£å®Ÿè¡Œãªã©ã®æœ¬ç•ªå¯¾å¿œæ©Ÿèƒ½ã‚’å‚™ãˆã¦ã„ã¾ã™ãŒã€ä¿¡é ¼æ€§ã€ä¿å®ˆæ€§ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®æ”¹å–„ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚

### ç¾çŠ¶åˆ†æ

**å¼·ã¿:**
- âœ… åŒ…æ‹¬çš„ãªæ©Ÿèƒ½ã‚»ãƒƒãƒˆï¼ˆHITLã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã€åˆ†æ•£å®Ÿè¡Œï¼‰
- âœ… ãƒ¢ãƒ€ãƒ³ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹é€ ã¨æ˜ç¢ºãªé–¢å¿ƒã®åˆ†é›¢
- âœ… é©åˆ‡ãªãƒ­ã‚°å®Ÿè£…ï¼ˆprint()æ–‡ãªã—ï¼‰
- âœ… ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªé–‹ç™ºã¨æœ€è¿‘ã®æ”¹å–„
- âœ… è‰¯å¥½ãªãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸

**æ”¹å–„ãŒå¿…è¦ãªé ˜åŸŸ:**
- âš ï¸ åºƒç¯„ãªä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©: 6-73å€‹
- âš ï¸ Redisã®æœ¬ç•ªå¯¾å¿œã®å•é¡Œï¼ˆKEYSä½¿ç”¨ã€ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ä¸è¶³ï¼‰
- âš ï¸ HITLã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®ãƒˆãƒ¬ãƒ¼ã‚¹æœªå®Œäº†
- âš ï¸ åˆ†æ•£ã‚­ãƒ¥ãƒ¼ã®è€ä¹…æ€§ä¸è¶³ï¼ˆDLQæœªå®Ÿè£…ï¼‰
- âš ï¸ ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ™‚ã®ãƒªã‚«ãƒãƒªé…å»¶ï¼ˆ30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰
- âš ï¸ ExecutionContextã®è‚¥å¤§åŒ–ï¼ˆ~1400è¡Œï¼‰
- âš ï¸ æ–°æ©Ÿèƒ½ã®çµ±åˆãƒ†ã‚¹ãƒˆä¸è¶³

---

## å„ªå…ˆåº¦åˆ¥æ”¹å–„ææ¡ˆ

### ğŸ”´ æœ€å„ªå…ˆï¼ˆHigh Priorityï¼‰

#### 1. HITLã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®ãƒˆãƒ¬ãƒ¼ã‚¹çµ‚äº†

**å•é¡Œ:**
- `WorkflowEngine.execute`ãŒ`FeedbackTimeoutError`ã§æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³ã™ã‚‹éš›ã€`tracer.on_workflow_end`ã‚’å‘¼ã°ãšã«çµ‚äº†
- ãƒ«ãƒ¼ãƒˆã‚¹ãƒ‘ãƒ³ãŒé–‹ã„ãŸã¾ã¾ã«ãªã‚Šã€Langfuseãƒˆãƒ¬ãƒ¼ã‚¹ãŒä¸å®Œå…¨ã«
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆæ™‚ã«ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ã‚¤ãƒ™ãƒ³ãƒˆãŒè¨˜éŒ²ã•ã‚Œãªã„

**å½±éŸ¿:**
- ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œæ™‚ã®ãƒˆãƒ¬ãƒ¼ã‚¹åˆ†æãŒå›°é›£
- åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®æ•´åˆæ€§ãŒå´©ã‚Œã‚‹

**å¯¾ç­–:**

```python
# graflow/core/engine.py ã® execute() ãƒ¡ã‚½ãƒƒãƒ‰

def execute(
    self,
    context: ExecutionContext,
    start_task_id: Optional[str] = None
) -> Any:
    """Execute workflow or single task using the provided context."""
    assert context.graph is not None, "Graph must be set before execution"

    workflow_name = getattr(context.graph, 'name', None) or f"workflow_{context.session_id[:8]}"

    # ãƒˆãƒ¬ãƒ¼ã‚¹é–‹å§‹ï¼ˆãƒã‚¹ãƒˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    if context.parent_context is None:
        context.tracer.on_workflow_start(workflow_name, context)

    try:
        # ... æ—¢å­˜ã®å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯ ...

        # FeedbackTimeoutErrorå‡¦ç†
        except Exception as e:
            from graflow.hitl.types import FeedbackTimeoutError

            if isinstance(e, FeedbackTimeoutError):
                self._handle_feedback_timeout(e, task_id, task, context)

                # ========== è¿½åŠ : ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®ãƒˆãƒ¬ãƒ¼ã‚¹çµ‚äº† ==========
                if context.parent_context is None:
                    context.tracer.on_workflow_end(
                        workflow_name,
                        context,
                        result=None,
                        metadata={
                            "status": "timeout",
                            "feedback_id": e.feedback_id,
                            "checkpoint_path": context.last_checkpoint_path
                        }
                    )
                # =====================================================
                return None

            raise exceptions.as_runtime_error(e)

    finally:
        # ========== è¿½åŠ : å¿…ãš on_workflow_end ã‚’å‘¼ã¶ ==========
        # æ—¢ã«å‘¼ã°ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆäºŒé‡å‘¼ã³å‡ºã—é˜²æ­¢ï¼‰
        # ã¾ãŸã¯ã€ãƒˆãƒ¬ãƒ¼ã‚µãƒ¼å´ã§äºŒé‡å‘¼ã³å‡ºã—ã‚’é˜²ããƒ•ãƒ©ã‚°ã‚’å®Ÿè£…
        # ===================================================
        pass
```

**è¿½åŠ ãƒ†ã‚¹ãƒˆ:**

```python
# tests/trace/test_hitl_timeout_tracing.py

def test_tracer_completes_on_hitl_timeout(tmp_path, mock_tracer):
    """HITLã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã‚‚ãƒˆãƒ¬ãƒ¼ã‚µãƒ¼ãŒæ­£ã—ãçµ‚äº†ã™ã‚‹ã“ã¨ã‚’ç¢ºèª."""
    with workflow("hitl_timeout_test") as wf:
        @task(inject_context=True)
        def timeout_task(ctx):
            ctx.request_approval("Approve?", timeout=0.1)

        wf.add_task(timeout_task)

        context = wf.get_context()
        context.tracer = mock_tracer

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§çµ‚äº†
        try:
            wf.execute()
        except FeedbackTimeoutError:
            pass

        # on_workflow_end ãŒå‘¼ã°ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
        assert mock_tracer.on_workflow_end.called
        call_args = mock_tracer.on_workflow_end.call_args
        assert call_args[1]["metadata"]["status"] == "timeout"
```

**å·¥æ•°:** 1æ—¥
**å„ªå…ˆåº¦:** æœ€å„ªå…ˆ - æœ¬ç•ªç’°å¢ƒã§ã®éšœå®³åˆ†æã«å½±éŸ¿

---

#### 2. åºƒç¯„ãªä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©ã®ç½®ãæ›ãˆ

**ç¾çŠ¶:** 6-73å€‹ã®åºƒç¯„ãª`except Exception:`ã¾ãŸã¯`except BaseException:`ãƒ–ãƒ­ãƒƒã‚¯

**ä¸»ãªç®‡æ‰€:**
- `trace/langfuse.py`: 9å€‹
- `hitl/backend/redis.py`: 6å€‹
- `coordination/threading_coordinator.py`: 3å€‹
- `worker/worker.py`: 4å€‹
- `llm/client.py`: 4å€‹
- `channels/redis_channel.py`: pingå‡¦ç†
- `core/engine.py`: execute catch-all

**å•é¡Œ:**
- ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°ã—ã¦æ¡ã‚Šã¤ã¶ã™ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ã‚¹ãŒç™ºç”Ÿ
- ãƒ‡ãƒãƒƒã‚°ãŒå›°é›£

**å¯¾ç­–:**

```python
# æ‚ªã„ä¾‹ï¼ˆç¾çŠ¶ï¼‰
try:
    result = execute_task()
except Exception as e:  # åºƒã™ãã‚‹ï¼
    logger.error(f"Error: {e}")
    return None

# è‰¯ã„ä¾‹ï¼ˆæ”¹å–„å¾Œï¼‰
from graflow.exceptions import TaskExecutionError, TaskTimeoutError
import redis
import json

try:
    result = execute_task()
except TaskTimeoutError as e:
    logger.warning(
        "Task timed out, will retry",
        extra={"task_id": task.task_id, "timeout": e.timeout}
    )
    return retry_task(task)
except TaskExecutionError as e:
    logger.error(
        "Task execution failed",
        extra={"task_id": task.task_id, "error": str(e)},
        exc_info=True
    )
    raise
except redis.RedisError as e:
    logger.error(
        "Redis connection error",
        extra={"host": redis_config.host, "error": str(e)}
    )
    raise TaskExecutionError(f"Redis error: {e}") from e
except json.JSONDecodeError as e:
    logger.error(
        "Invalid JSON in task payload",
        extra={"payload": payload[:100], "error": str(e)}
    )
    raise TaskExecutionError(f"JSON decode error: {e}") from e
# Exception ã‚„ BaseException ã¯ä½¿ã‚ãªã„
```

**ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚³ãƒ¼ãƒ‰å‘ã‘ã®ä¾‹å¤–å‡¦ç†:**

```python
def shutdown(self):
    """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆä¾‹å¤–ã‚’æ¡ã‚Šã¤ã¶ã™å¿…è¦ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰."""
    try:
        self._cleanup_resources()
    except BaseException as e:
        logger.error("Error during shutdown cleanup", exc_info=True)
        # KeyboardInterrupt ã¨ SystemExit ã¯å†åº¦raiseã™ã‚‹
        if isinstance(e, (KeyboardInterrupt, SystemExit)):
            raise
```

**ç›®æ¨™:** ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å°‚ç”¨ä»¥å¤–ã®åºƒç¯„ãªä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©ã‚’0ã«

**å·¥æ•°:** 2-3æ—¥
**å„ªå…ˆåº¦:** æœ€å„ªå…ˆ - ä¿¡é ¼æ€§ã¨ãƒ‡ãƒãƒƒã‚°æ€§å‘ä¸Š

---

#### 3. Redisã®æœ¬ç•ªå¯¾å¿œï¼ˆãƒ˜ãƒ«ã‚¹ã‚·ã‚°ãƒŠãƒ«è¿½åŠ ï¼‰

**å•é¡Œ:**

2. **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ä¸è¶³:**
   - `RedisChannel.ping()`: å…¨ä¾‹å¤–ã‚’æ¡ã‚Šã¤ã¶ã—ã¦`False`ã‚’è¿”ã™ã ã‘
   - éšœå®³ãŒéš è”½ã•ã‚Œã‚‹

3. **åŠ¹ç‡ã®æ‚ªã„ãƒªã‚¹ãƒˆæ“ä½œ:**
   - ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸€è¦§ã‚’`keys() + GET + JSON parse`ã§å–å¾—
   - OOMã‚„ã‚¹ãƒˆãƒ¼ãƒ«ã®ãƒªã‚¹ã‚¯

**å¯¾ç­–:**

**2) ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã‚»ãƒƒãƒˆã§ç®¡ç†:**

```python
# graflow/hitl/backend/redis.py

class RedisHITLBackend:
    """Redisãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰."""

    def create_request(self, request: FeedbackRequest) -> str:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ."""
        request_id = request.feedback_id

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’JSONä¿å­˜
        key = f"{self.key_prefix}:request:{request_id}"
        self._redis.setex(
            key,
            self.request_ttl,
            json.dumps(request.to_dict(), default=str)
        )

        # ========== è¿½åŠ : ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ  ==========
        # ã‚¹ã‚³ã‚¢: ä½œæˆæ™‚åˆ»ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰
        # ãƒ¡ãƒ³ãƒãƒ¼: feedback_id
        index_key = f"{self.key_prefix}:index:requests"
        self._redis.zadd(
            index_key,
            {request_id: request.created_at.timestamp()}
        )
        # ===========================================

        return request_id

    def list_pending_requests(
        self,
        limit: int = 100,
        offset: int = 0
    ) -> List[FeedbackRequest]:
        """ä¿ç•™ä¸­ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒ³ã‚°å¯¾å¿œï¼‰."""
        index_key = f"{self.key_prefix}:index:requests"

        # ========== æœ€é©åŒ–: ZREVRANGE ã§ãƒšãƒ¼ã‚¸ãƒ³ã‚°å–å¾— ==========
        # æœ€æ–°ã®ã‚‚ã®ã‹ã‚‰å–å¾—ï¼ˆé™é †ï¼‰
        request_ids = self._redis.zrevrange(
            index_key,
            offset,
            offset + limit - 1
        )
        # ====================================================

        requests = []
        for request_id in request_ids:
            if isinstance(request_id, bytes):
                request_id = request_id.decode('utf-8')

            request = self.get_request(request_id)
            if request and request.status == FeedbackStatus.PENDING:
                requests.append(request)

        return requests
```

**3) pingã®ãƒ­ã‚®ãƒ³ã‚°æ”¹å–„:**

```python
# graflow/channels/redis_channel.py

def ping(self) -> bool:
    """Redisæ¥ç¶šã®ç¢ºèªï¼ˆãƒ­ã‚®ãƒ³ã‚°ä»˜ãï¼‰."""
    try:
        result = self._redis.ping()
        return result
    except redis.RedisError as e:
        logger.error(
            "Redis ping failed",
            extra={
                "host": self._redis.connection_pool.connection_kwargs.get("host"),
                "port": self._redis.connection_pool.connection_kwargs.get("port"),
                "error": str(e)
            }
        )
        return False
    except Exception as e:
        logger.error(
            "Unexpected error during Redis ping",
            extra={"error": str(e)},
            exc_info=True
        )
        return False

def health_check(self) -> Dict[str, Any]:
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æƒ…å ±ã‚’å–å¾—."""
    try:
        info = self._redis.info()
        return {
            "status": "healthy",
            "connected_clients": info.get("connected_clients"),
            "used_memory": info.get("used_memory_human"),
            "uptime_seconds": info.get("uptime_in_seconds")
        }
    except redis.RedisError as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }
```

**ç›®æ¨™:** Redis `KEYS`ä½¿ç”¨ç®‡æ‰€ã‚’0ã«

**å·¥æ•°:** 3-4æ—¥
**å„ªå…ˆåº¦:** æœ€å„ªå…ˆ - æœ¬ç•ªç’°å¢ƒã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

---

#### 4. åˆ†æ•£ã‚­ãƒ¥ãƒ¼ã®è€ä¹…æ€§è¿½åŠ ï¼ˆDLQå®Ÿè£…ï¼‰

**å•é¡Œ:**
- `DistributedTaskQueue.dequeue`ãŒãƒ‘ãƒ¼ã‚¹ä¸å¯èƒ½ãªã‚¢ã‚¤ãƒ†ãƒ ã‚’è­¦å‘Šå¾Œã«ç ´æ£„
- ã‚°ãƒ©ãƒ•ã‚¹ãƒˆã‚¢ä¸åœ¨æ™‚ã‚‚ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ã¿ã§ç ´æ£„
- å¯è¦–æ€§ãŒãªãã€ãƒãƒ«ãƒãƒ¯ãƒ¼ã‚«ãƒ¼ç’°å¢ƒã§ã®ãƒ‡ãƒãƒƒã‚°ãŒå›°é›£

**å¯¾ç­–:**

```python
# graflow/queue/distributed.py

class DistributedTaskQueue:
    """åˆ†æ•£ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ï¼ˆDLQå¯¾å¿œç‰ˆï¼‰."""

    def __init__(
        self,
        redis_client: redis.Redis,
        key_prefix: str = "graflow",
        dlq_ttl: int = 86400 * 7  # 7æ—¥é–“ä¿æŒ
    ):
        self._redis = redis_client
        self._key_prefix = key_prefix
        self._queue_key = f"{key_prefix}:queue"
        self._dlq_key = f"{key_prefix}:dlq"  # Dead Letter Queue
        self._dlq_ttl = dlq_ttl

        # ========== è¿½åŠ : ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ ==========
        self._metrics = {
            "dequeued": 0,
            "decoded": 0,
            "dropped": 0,
            "dlq_sent": 0
        }
        # ==========================================

    def _send_to_dlq(
        self,
        payload: Union[str, bytes],
        reason: str,
        error: Optional[Exception] = None
    ) -> None:
        """Dead Letter Queueã«ã‚¢ã‚¤ãƒ†ãƒ ã‚’é€ä¿¡."""
        dlq_item = {
            "payload": payload if isinstance(payload, str) else payload.decode('utf-8'),
            "reason": reason,
            "error": str(error) if error else None,
            "timestamp": datetime.now().isoformat(),
            "queue_key": self._queue_key
        }

        # DLQã«è¿½åŠ ï¼ˆTTLä»˜ãï¼‰
        dlq_key = f"{self._dlq_key}:{uuid.uuid4().hex}"
        self._redis.setex(
            dlq_key,
            self._dlq_ttl,
            json.dumps(dlq_item)
        )

        self._metrics["dlq_sent"] += 1

        logger.error(
            "Task sent to DLQ",
            extra={
                "dlq_key": dlq_key,
                "reason": reason,
                "error": str(error) if error else None,
                "payload_preview": str(payload)[:200]
            }
        )

    def dequeue(self) -> Optional[TaskSpec]:
        """ã‚¿ã‚¹ã‚¯ã‚’ãƒ‡ã‚­ãƒ¥ãƒ¼ï¼ˆDLQå¯¾å¿œç‰ˆï¼‰."""
        payload = self._redis.lpop(self._queue_key)
        if not payload:
            return None

        self._metrics["dequeued"] += 1

        try:
            data = json.loads(payload)
            self._metrics["decoded"] += 1
        except json.JSONDecodeError as e:
            self._metrics["dropped"] += 1
            # ========== å¤‰æ›´: DLQã«é€ä¿¡ ==========
            self._send_to_dlq(payload, "json_decode_error", e)
            # ==================================
            return None

        # ã‚°ãƒ©ãƒ•ã‚¹ãƒˆã‚¢ãƒã‚§ãƒƒã‚¯
        graph_store = get_global_graph_store()
        if graph_store is None:
            self._metrics["dropped"] += 1
            # ========== å¤‰æ›´: DLQã«é€ä¿¡ ==========
            self._send_to_dlq(
                payload,
                "graph_store_not_available",
                RuntimeError("Graph store not initialized")
            )
            # ==================================
            return None

        # ... æ—¢å­˜ã®TaskSpecä½œæˆãƒ­ã‚¸ãƒƒã‚¯ ...

    def get_metrics(self) -> Dict[str, int]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—."""
        return self._metrics.copy()

    def list_dlq_items(
        self,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """DLQã‚¢ã‚¤ãƒ†ãƒ ã‚’ä¸€è¦§è¡¨ç¤º."""
        pattern = f"{self._dlq_key}:*"
        items = []

        for key in self._redis.scan_iter(match=pattern, count=100):
            if len(items) >= limit:
                break

            data = self._redis.get(key)
            if data:
                try:
                    items.append(json.loads(data))
                except json.JSONDecodeError:
                    continue

        return items
```

**ãƒ†ã‚¹ãƒˆè¿½åŠ :**

```python
# tests/queue/test_distributed_dlq.py

def test_malformed_json_sent_to_dlq(redis_client):
    """ä¸æ­£ãªJSONãŒDLQã«é€ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª."""
    queue = DistributedTaskQueue(redis_client, key_prefix="test")

    # ä¸æ­£ãªJSONã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
    redis_client.rpush(queue._queue_key, "{invalid json}")

    # ãƒ‡ã‚­ãƒ¥ãƒ¼è©¦è¡Œ
    result = queue.dequeue()
    assert result is None

    # DLQã«é€ã‚‰ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
    dlq_items = queue.list_dlq_items()
    assert len(dlq_items) == 1
    assert dlq_items[0]["reason"] == "json_decode_error"

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç¢ºèª
    metrics = queue.get_metrics()
    assert metrics["dlq_sent"] == 1
    assert metrics["dropped"] == 1
```

**ç›®æ¨™:** DLQã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®Ÿè£…å®Œäº†

**å·¥æ•°:** 2-3æ—¥
**å„ªå…ˆåº¦:** æœ€å„ªå…ˆ - æœ¬ç•ªç’°å¢ƒã§ã®å¯è¦–æ€§ã¨ãƒ‡ãƒãƒƒã‚°æ€§

---

#### 5. ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆå®Ÿè£…

**å•é¡Œ:**
- `RedisCoordinator.wait_barrier`ãŒãƒ¯ãƒ¼ã‚«ãƒ¼ã®å®Œäº†ã‚’ç›²ç›®çš„ã«å¾…æ©Ÿ
- ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ï¼ˆOOM/Segfaultï¼‰ã—ãŸå ´åˆã€30ç§’ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ã§å¾…æ©Ÿ
- æ—©æœŸæ¤œå‡ºãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒãªã„

**å¯¾ç­–:**

```python
# graflow/worker/heartbeat.py ï¼ˆæ–°è¦ä½œæˆï¼‰

import threading
import time
import redis
from typing import Optional
import logging

logger = logging.getLogger(__name__)

class WorkerHeartbeat:
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆç®¡ç†."""

    def __init__(
        self,
        redis_client: redis.Redis,
        task_id: str,
        key_prefix: str = "graflow",
        interval: int = 5,  # 5ç§’ã”ã¨
        ttl: int = 15  # 15ç§’ã®TTL
    ):
        self._redis = redis_client
        self._task_id = task_id
        self._key = f"{key_prefix}:heartbeat:{task_id}"
        self._interval = interval
        self._ttl = ttl
        self._thread: Optional[threading.Thread] = None
        self._stop_event = threading.Event()

    def start(self) -> None:
        """ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚’é–‹å§‹."""
        if self._thread is not None:
            logger.warning("Heartbeat already started")
            return

        self._stop_event.clear()
        self._thread = threading.Thread(
            target=self._heartbeat_loop,
            daemon=True,
            name=f"heartbeat-{self._task_id}"
        )
        self._thread.start()
        logger.debug(f"Heartbeat started for task: {self._task_id}")

    def stop(self) -> None:
        """ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚’åœæ­¢."""
        if self._thread is None:
            return

        self._stop_event.set()
        self._thread.join(timeout=self._interval + 1)
        self._thread = None

        # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚­ãƒ¼ã‚’å‰Šé™¤
        try:
            self._redis.delete(self._key)
        except redis.RedisError as e:
            logger.warning(f"Failed to delete heartbeat key: {e}")

        logger.debug(f"Heartbeat stopped for task: {self._task_id}")

    def _heartbeat_loop(self) -> None:
        """ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ—ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰."""
        while not self._stop_event.is_set():
            try:
                # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚­ãƒ¼ã‚’æ›´æ–°ï¼ˆTTLä»˜ãï¼‰
                self._redis.setex(
                    self._key,
                    self._ttl,
                    time.time()
                )
            except redis.RedisError as e:
                logger.error(f"Failed to update heartbeat: {e}")

            # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«å¾…æ©Ÿï¼ˆæ—©æœŸåœæ­¢å¯èƒ½ï¼‰
            self._stop_event.wait(self._interval)

    def __enter__(self):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é–‹å§‹."""
        self.start()
        return self

    def __exit__(self, *args):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ‚äº†."""
        self.stop()
```

**ãƒ¯ãƒ¼ã‚«ãƒ¼å´ã§ã®ä½¿ç”¨:**

```python
# graflow/worker/worker.py

def execute_task(self, task_spec: TaskSpec) -> None:
    """ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆä»˜ãï¼‰."""
    task_id = task_spec.executable.task_id

    # ========== è¿½åŠ : ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé–‹å§‹ ==========
    with WorkerHeartbeat(
        self._redis,
        task_id,
        key_prefix=self._key_prefix
    ):
        # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        try:
            result = task_spec.executable.run()
            self._report_success(task_id, result)
        except Exception as e:
            self._report_failure(task_id, e)
            raise
    # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆè‡ªå‹•åœæ­¢
    # ========================================
```

**ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼å´ã§ã®ãƒã‚§ãƒƒã‚¯:**

```python
# graflow/coordination/redis_coordinator.py

def wait_barrier(
    self,
    group_id: str,
    task_count: int,
    timeout: float = 30.0
) -> bool:
    """ãƒãƒªã‚¢å¾…æ©Ÿï¼ˆãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰."""
    start_time = time.time()
    check_interval = 1.0  # 1ç§’ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
    heartbeat_timeout = 10.0  # 10ç§’é–“ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãªã—ã§å¤±æ•—

    while time.time() - start_time < timeout:
        # å®Œäº†æ•°ã‚’ãƒã‚§ãƒƒã‚¯
        completion_count = self._get_completion_count(group_id)
        if completion_count >= task_count:
            return True

        # ========== è¿½åŠ : ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ ==========
        # å®Ÿè¡Œä¸­ã‚¿ã‚¹ã‚¯ã®ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚’ç¢ºèª
        running_tasks = self._get_running_tasks(group_id)
        for task_id in running_tasks:
            heartbeat_key = f"{self._key_prefix}:heartbeat:{task_id}"

            # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
            if not self._redis.exists(heartbeat_key):
                logger.error(
                    f"Task {task_id} heartbeat missing - worker may have crashed",
                    extra={"group_id": group_id, "task_id": task_id}
                )
                # å³åº§ã«å¤±æ•—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¾…ãŸãªã„ï¼‰
                return False
        # ===========================================

        time.sleep(check_interval)

    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    logger.error(f"Barrier timeout for group: {group_id}")
    return False
```

**ç›®æ¨™:** ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ™‚ã®ãƒªã‚«ãƒãƒªæ™‚é–“ã‚’30ç§’â†’10ç§’æœªæº€ã«çŸ­ç¸®

**å·¥æ•°:** 3æ—¥
**å„ªå…ˆåº¦:** æœ€å„ªå…ˆ - æœ¬ç•ªç’°å¢ƒã§ã®å›å¾©åŠ›å‘ä¸Š

---

#### 6. çµ±åˆãƒ†ã‚¹ãƒˆã®æ‹¡å……

**ç¾çŠ¶:** HITLã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°æ©Ÿèƒ½ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒä¸è¶³

**ä¸è¶³ã—ã¦ã„ã‚‹é ˜åŸŸ:**
- HITLãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ 
- Redisãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆãƒ»å¾©å…ƒ
- Langfuseãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°çµ±åˆã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰
- LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±åˆï¼ˆãƒ¢ãƒƒã‚¯ä½¿ç”¨ï¼‰
- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

**å¯¾ç­–:**

```python
# tests/integration/test_hitl_redis_integration.py

import pytest
import redis
from graflow.hitl.manager import FeedbackManager
from graflow.hitl.types import FeedbackType, FeedbackStatus
from graflow.core.checkpoint import CheckpointManager

@pytest.mark.integration
class TestHITLRedisIntegration:
    """HITL Redisçµ±åˆãƒ†ã‚¹ãƒˆ."""

    @pytest.fixture
    def redis_client(self):
        """Redisã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£."""
        client = redis.Redis(
            host="localhost",
            port=6379,
            decode_responses=True
        )
        # ãƒ†ã‚¹ãƒˆå‰ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for key in client.scan_iter("test:*"):
            client.delete(key)
        yield client
        # ãƒ†ã‚¹ãƒˆå¾Œã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for key in client.scan_iter("test:*"):
            client.delete(key)

    def test_timeout_checkpoint_resume_with_redis(
        self,
        redis_client,
        tmp_path
    ):
        """Redisä½¿ç”¨æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆâ†’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆâ†’ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ã‚’ç¢ºèª."""

        # 1. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’Redisãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ä½œæˆ
        feedback_manager = FeedbackManager(
            backend="redis",
            backend_config={"redis_client": redis_client, "key_prefix": "test"}
        )

        # 2. ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã™ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆ
        with workflow("redis_hitl_test") as wf:
            @task(inject_context=True)
            def approval_task(ctx):
                ctx.feedback_manager = feedback_manager
                response = ctx.request_approval(
                    prompt="Approve deployment?",
                    timeout=1.0  # 1ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                )
                return response

            wf.add_task(approval_task)
            context = wf.get_context()

            # 3. å®Ÿè¡Œã—ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            try:
                wf.execute()
                pytest.fail("Should have timed out")
            except FeedbackTimeoutError as e:
                feedback_id = e.feedback_id

            # 4. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒä½œæˆã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
            assert context.last_checkpoint_path is not None
            checkpoint_path = context.last_checkpoint_path
            assert Path(checkpoint_path).exists()

            # 5. Redisã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            request = feedback_manager.backend.get_request(feedback_id)
            assert request is not None
            assert request.status == FeedbackStatus.PENDING

        # 6. å¤–éƒ¨ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ï¼ˆåˆ¥ãƒ¯ãƒ¼ã‚«ãƒ¼æƒ³å®šï¼‰
        feedback_manager.respond_to_feedback(
            feedback_id=feedback_id,
            response={"approved": True}
        )

        # 7. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ 
        resumed_context, metadata = CheckpointManager.resume_from_checkpoint(
            checkpoint_path
        )
        resumed_context.feedback_manager = feedback_manager

        # 8. å®Ÿè¡Œå®Œäº†ã‚’ç¢ºèª
        engine = WorkflowEngine()
        engine.execute(resumed_context)

        # 9. çµæœã‚’ç¢ºèª
        result = resumed_context.get_result(approval_task.task_id)
        assert result is True

        # 10. Redisã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒå®Œäº†ã«ãªã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        request = feedback_manager.backend.get_request(feedback_id)
        assert request.status == FeedbackStatus.COMPLETED
```

```python
# tests/integration/test_langfuse_tracing.py

@pytest.mark.integration
class TestLangfuseTracingIntegration:
    """Langfuseãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆ."""

    def test_distributed_tracing_with_parallel_group(self, mock_langfuse):
        """ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œæ™‚ã®åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’ç¢ºèª."""
        # ... ãƒ†ã‚¹ãƒˆå®Ÿè£… ...
```

**ç›®æ¨™:** çµ±åˆãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’åŒ…æ‹¬çš„ã«

**å·¥æ•°:** 2é€±é–“
**å„ªå…ˆåº¦:** æœ€å„ªå…ˆ - æœ¬ç•ªç’°å¢ƒã§ã®ä¿¡é ¼æ€§

---

### ğŸŸ¡ ä¸­å„ªå…ˆï¼ˆMedium Priorityï¼‰

#### 7. ExecutionContextã®åˆ†è§£

**ç¾çŠ¶:** ExecutionContextãŒç´„1400è¡Œã§å¤šãã®è²¬å‹™ã‚’æŒã¤

**å•é¡Œ:**
- ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£ã®ä½ä¸‹
- ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã®å›°é›£ã•
- è¤‡æ•°ã®ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ ã¸ã®çµåˆ

**å¯¾ç­–:**

```python
# graflow/core/context_managers.py ï¼ˆæ–°è¦ä½œæˆï¼‰

from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List
from pathlib import Path

@dataclass
class CheckpointState:
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆçŠ¶æ…‹ç®¡ç†."""
    last_checkpoint_path: Optional[Path] = None
    checkpoint_metadata: Dict[str, Any] = field(default_factory=dict)
    checkpoint_requested: bool = False
    checkpoint_request_metadata: Optional[Dict[str, Any]] = None
    checkpoint_request_path: Optional[str] = None
    completed_tasks: List[str] = field(default_factory=list)

    def request_checkpoint(
        self,
        metadata: Optional[Dict] = None,
        path: Optional[str] = None
    ) -> None:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆ."""
        self.checkpoint_requested = True
        self.checkpoint_request_metadata = dict(metadata) if metadata else {}
        self.checkpoint_request_path = path

    def clear_request(self) -> None:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢."""
        self.checkpoint_requested = False
        self.checkpoint_request_metadata = None
        self.checkpoint_request_path = None

    def mark_task_completed(self, task_id: str) -> None:
        """ã‚¿ã‚¹ã‚¯å®Œäº†ã‚’è¨˜éŒ²."""
        if task_id not in self.completed_tasks:
            self.completed_tasks.append(task_id)

@dataclass
class LLMRegistry:
    """LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç®¡ç†."""
    _llm_client: Optional[Any] = None
    _llm_agents: Dict[str, Any] = field(default_factory=dict)
    _llm_agents_yaml: Dict[str, str] = field(default_factory=dict)

    def register_agent(self, name: str, agent: Any) -> None:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç™»éŒ²."""
        self._llm_agents[name] = agent

        # ADKã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å ´åˆã¯YAMLã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
        try:
            from graflow.llm.agents.adk_agent import AdkLLMAgent
            from graflow.llm.serialization import agent_to_yaml

            if isinstance(agent, AdkLLMAgent):
                yaml_str = agent_to_yaml(agent._adk_agent)
                self._llm_agents_yaml[name] = yaml_str
        except (ImportError, AttributeError):
            pass

    def get_agent(self, name: str) -> Any:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å–å¾—ï¼ˆé…å»¶å¾©å…ƒå¯¾å¿œï¼‰."""
        if name in self._llm_agents:
            return self._llm_agents[name]

        # YAMLã‹ã‚‰å¾©å…ƒ
        if name in self._llm_agents_yaml:
            try:
                from graflow.llm.agents.adk_agent import AdkLLMAgent
                from graflow.llm.serialization import yaml_to_agent

                adk_agent = yaml_to_agent(self._llm_agents_yaml[name])
                agent = AdkLLMAgent._from_adk_agent(adk_agent, "")
                self._llm_agents[name] = agent
                return agent
            except (ImportError, Exception):
                pass

        raise KeyError(f"LLMAgent '{name}' not found in registry")

    @property
    def llm_client(self) -> Any:
        """LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰."""
        if self._llm_client is None:
            from graflow.llm.client import LLMClient
            import os

            default_model = os.getenv("GRAFLOW_LLM_MODEL", "gpt-5-mini")
            self._llm_client = LLMClient(model=default_model)

        return self._llm_client

@dataclass
class GraphNavigator:
    """ã‚°ãƒ©ãƒ•ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«ã¨ã‚­ãƒ¥ãƒ¼ç®¡ç†."""
    graph: Any  # TaskGraph
    queue: Any  # LocalTaskQueue
    start_node: Optional[str] = None

    def get_next_task(self) -> Optional[str]:
        """æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—."""
        return self.queue.get_next_task()

    def add_to_queue(self, executable: Any) -> None:
        """ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ."""
        from graflow.queue.base import TaskSpec

        task_spec = TaskSpec(
            executable=executable,
            execution_context=None,  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯å¾Œã§è¨­å®š
            trace_id="",  # å¾Œã§è¨­å®š
            parent_span_id=None
        )
        self.queue.enqueue(task_spec)
```

**ExecutionContextã§ã®ä½¿ç”¨:**

```python
# graflow/core/context.py

class ExecutionContext:
    """å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰."""

    def __init__(self, ...):
        # ... æ—¢å­˜ã®åˆæœŸåŒ– ...

        # ========== è¿½åŠ : å°‚é–€ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ä½¿ç”¨ ==========
        self._checkpoint_state = CheckpointState()
        self._llm_registry = LLMRegistry()
        self._graph_navigator = GraphNavigator(self.graph, self.task_queue, start_node)
        # ==============================================

    # ãƒ‡ãƒªã‚²ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰
    def request_checkpoint(
        self,
        metadata: Optional[Dict] = None,
        path: Optional[str] = None
    ) -> None:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆ."""
        return self._checkpoint_state.request_checkpoint(metadata, path)

    def mark_task_completed(self, task_id: str) -> None:
        """ã‚¿ã‚¹ã‚¯å®Œäº†ã‚’è¨˜éŒ²."""
        return self._checkpoint_state.mark_task_completed(task_id)

    def register_llm_agent(self, name: str, agent: Any) -> None:
        """LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç™»éŒ²."""
        return self._llm_registry.register_agent(name, agent)

    def get_llm_agent(self, name: str) -> Any:
        """LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å–å¾—."""
        return self._llm_registry.get_agent(name)

    @property
    def llm_client(self) -> Any:
        """LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—."""
        return self._llm_registry.llm_client
```

**ç›®æ¨™:** ExecutionContextã‚’800è¡Œæœªæº€ã«å‰Šæ¸›

**å·¥æ•°:** 1é€±é–“
**å„ªå…ˆåº¦:** ä¸­ - ä¿å®ˆæ€§å‘ä¸Šï¼ˆç¾çŠ¶ã¯è¨±å®¹ç¯„å›²ï¼‰

---

#### 8. ã‚°ãƒ©ãƒ•ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–ï¼ˆCASå®Ÿè£…ï¼‰

**å•é¡Œ:**
- `RedisCoordinator.execute_group`ãŒã‚°ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œã”ã¨ã«ç„¡æ¡ä»¶ã§TaskGraphã‚’Redisã«ä¿å­˜
- å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã‚„é »ç¹ãªä¸¦åˆ—ã‚¹ãƒ†ãƒƒãƒ—ã§I/Oã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

**å¯¾ç­–:**

```python
# graflow/core/graph.py

class TaskGraph:
    """ã‚¿ã‚¹ã‚¯ã‚°ãƒ©ãƒ•ï¼ˆãƒãƒƒã‚·ãƒ¥è¨ˆç®—å¯¾å¿œç‰ˆï¼‰."""

    def calculate_hash(self) -> str:
        """ã‚°ãƒ©ãƒ•æ§‹é€ ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆContent-Addressable Storageç”¨ï¼‰."""
        import hashlib
        import json

        # ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
        structure = {
            "nodes": sorted(self._graph.nodes()),
            "edges": sorted([
                (u, v, self._graph.edges[u, v].get("relation", ""))
                for u, v in self._graph.edges()
            ])
        }

        # JSONæ–‡å­—åˆ—åŒ–ã—ã¦ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        json_str = json.dumps(structure, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()
```

**RedisCoordinatorã§ã®æœ€é©åŒ–:**

```python
# graflow/coordination/redis_coordinator.py

class RedisCoordinator:
    """Redisã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ï¼ˆCASæœ€é©åŒ–ç‰ˆï¼‰."""

    def execute_group(
        self,
        group_id: str,
        tasks: List[Executable],
        exec_context: ExecutionContext,
        policy_instance: GroupExecutionPolicy
    ) -> None:
        """ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œï¼ˆã‚°ãƒ©ãƒ•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æœ€é©åŒ–ç‰ˆï¼‰."""

        # ========== è¿½åŠ : Content-Addressable Storage ==========
        # ã‚°ãƒ©ãƒ•ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        graph = exec_context.graph
        graph_hash = graph.calculate_hash()

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒãƒƒã‚·ãƒ¥ã¨æ¯”è¼ƒ
        if not hasattr(exec_context, 'graph_hash'):
            exec_context.graph_hash = None

        # ãƒãƒƒã‚·ãƒ¥ãŒç•°ãªã‚‹ã‹ã€Redisã«å­˜åœ¨ã—ãªã„å ´åˆã®ã¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        if (graph_hash != exec_context.graph_hash or
            not self.graph_store.exists(graph_hash)):

            logger.debug(
                f"Uploading graph to Redis (hash: {graph_hash[:8]}...)",
                extra={"group_id": group_id}
            )
            self.graph_store.save(graph, key=graph_hash)
            exec_context.graph_hash = graph_hash
        else:
            logger.debug(
                f"Using cached graph (hash: {graph_hash[:8]}...)",
                extra={"group_id": group_id}
            )
        # ====================================================

        # ... æ—¢å­˜ã®ãƒãƒªã‚¢ã¨ã‚¿ã‚¹ã‚¯é€ä¿¡ãƒ­ã‚¸ãƒƒã‚¯ ...
```

**GraphStoreã®æ‹¡å¼µ:**

```python
# graflow/queue/graph_store.py

class RedisGraphStore:
    """Redisã‚°ãƒ©ãƒ•ã‚¹ãƒˆã‚¢ï¼ˆexistså¯¾å¿œç‰ˆï¼‰."""

    def exists(self, key: str) -> bool:
        """ã‚°ãƒ©ãƒ•ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯."""
        graph_key = f"{self._key_prefix}:graph:{key}"
        return self._redis.exists(graph_key) > 0

    def save(self, graph: TaskGraph, key: Optional[str] = None) -> str:
        """ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆã‚­ãƒ¼æŒ‡å®šå¯èƒ½ç‰ˆï¼‰."""
        if key is None:
            key = str(uuid.uuid4())

        graph_key = f"{self._key_prefix}:graph:{key}"
        # ... ä¿å­˜ãƒ­ã‚¸ãƒƒã‚¯ ...
        return key
```

**ç›®æ¨™:** ã‚°ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œã”ã¨ã®ã‚°ãƒ©ãƒ•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç‡ã‚’5%æœªæº€ã«

**å·¥æ•°:** 2æ—¥
**å„ªå…ˆåº¦:** ä¸­ - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š

---

#### 9. LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å›å¾©åŠ›å¼·åŒ–

**å•é¡Œ:**
- `LLMClient.completion`ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ/ãƒªãƒˆãƒ©ã‚¤åˆ¶å¾¡ãªã—ã§LiteLLMã«å§”è­²
- `extract_text`ãŒExceptionå…¨èˆ¬ã‚’ã‚­ãƒ£ãƒƒãƒã—ã¦ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™ï¼ˆã‚¨ãƒ©ãƒ¼éš è”½ï¼‰

**å¯¾ç­–:**

```python
# graflow/llm/client.py

from typing import Optional, Dict, Any, List
import logging
from tenacity import retry, stop_after_attempt, wait_exponential

logger = logging.getLogger(__name__)

class LLMClient:
    """LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå›å¾©åŠ›å¼·åŒ–ç‰ˆï¼‰."""

    def __init__(
        self,
        model: str = "gpt-5-mini",
        temperature: float = 0.7,
        timeout: float = 30.0,  # è¿½åŠ : ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        max_retries: int = 3  # è¿½åŠ : ãƒªãƒˆãƒ©ã‚¤å›æ•°
    ):
        self.model = model
        self.temperature = temperature
        self.timeout = timeout
        self.max_retries = max_retries

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        reraise=True
    )
    def completion(
        self,
        messages: List[Dict[str, str]],
        **kwargs: Any
    ) -> Any:
        """Completion APIã‚’å‘¼ã³å‡ºã™ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰."""
        import litellm

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        kwargs.setdefault("timeout", self.timeout)
        kwargs.setdefault("temperature", self.temperature)

        try:
            response = litellm.completion(
                model=self.model,
                messages=messages,
                **kwargs
            )
            return response
        except litellm.Timeout as e:
            logger.error(
                "LLM completion timeout",
                extra={
                    "model": self.model,
                    "timeout": self.timeout,
                    "error": str(e)
                }
            )
            raise
        except litellm.APIError as e:
            logger.error(
                "LLM API error",
                extra={
                    "model": self.model,
                    "status_code": getattr(e, 'status_code', None),
                    "error": str(e)
                }
            )
            raise

    def extract_text(self, response: Any) -> str:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„ç‰ˆï¼‰."""
        try:
            # Choicesã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            if hasattr(response, 'choices') and len(response.choices) > 0:
                choice = response.choices[0]
                if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                    return choice.message.content or ""

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–‡å­—åˆ—åŒ–
            logger.warning(
                "Unexpected response structure, falling back to str()",
                extra={"response_type": type(response).__name__}
            )
            return str(response)

        except AttributeError as e:
            # ========== å¤‰æ›´: ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°ã—ã¦ç©ºæ–‡å­—åˆ—ã§ã¯ãªãä¾‹å¤–ã‚’å†raise ==========
            logger.error(
                "Failed to extract text from response",
                extra={"error": str(e), "response": str(response)[:200]},
                exc_info=True
            )
            raise ValueError(f"Cannot extract text from response: {e}") from e
        # ========================================================================
```

**ãƒ†ã‚¹ãƒˆè¿½åŠ :**

```python
# tests/llm/test_client_resilience.py

def test_completion_retries_on_timeout(mock_litellm):
    """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã«ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã“ã¨ã‚’ç¢ºèª."""
    import litellm

    # æœ€åˆã®2å›ã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€3å›ç›®ã¯æˆåŠŸ
    mock_litellm.completion.side_effect = [
        litellm.Timeout("Timeout 1"),
        litellm.Timeout("Timeout 2"),
        {"choices": [{"message": {"content": "Success"}}]}
    ]

    client = LLMClient(max_retries=3)
    response = client.completion([{"role": "user", "content": "test"}])

    assert response["choices"][0]["message"]["content"] == "Success"
    assert mock_litellm.completion.call_count == 3

def test_extract_text_raises_on_invalid_response():
    """ä¸æ­£ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹ã“ã¨ã‚’ç¢ºèª."""
    client = LLMClient()

    with pytest.raises(ValueError, match="Cannot extract text"):
        client.extract_text({"invalid": "structure"})
```

**å·¥æ•°:** 2æ—¥
**å„ªå…ˆåº¦:** ä¸­ - ä¿¡é ¼æ€§å‘ä¸Š

---

#### 10. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¿½åŠ 

**ç¾çŠ¶:** ä½“ç³»çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãªã—

**ä¸è¶³ã—ã¦ã„ã‚‹é ˜åŸŸ:**
- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆtasks/secondï¼‰
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®šï¼ˆP50ã€P95ã€P99ï¼‰
- ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆï¼ˆ1 vs 10 vs 100ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰

**å¯¾ç­–:**

```python
# tests/performance/test_benchmarks.py

import pytest
import time
from statistics import mean, median, quantiles
from graflow.core.workflow import workflow
from graflow.core.decorators import task
from graflow.core.task import ParallelGroup

@pytest.mark.benchmark
class TestWorkflowPerformance:
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯."""

    def test_throughput_1000_simple_tasks(self, benchmark):
        """1000å€‹ã®å˜ç´”ã‚¿ã‚¹ã‚¯ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æ¸¬å®š."""

        @task
        def noop_task(i: int) -> int:
            """ä½•ã‚‚ã—ãªã„ã‚¿ã‚¹ã‚¯."""
            return i

        def run_workflow():
            with workflow("perf_test") as wf:
                tasks = [noop_task.clone(f"task-{i}") for i in range(1000)]
                parallel = ParallelGroup(tasks)
                wf.add_task(parallel)
                wf.execute()

        result = benchmark(run_workflow)

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã‚’ã‚¢ã‚µãƒ¼ãƒˆ
        tasks_per_second = 1000 / result.stats.mean
        assert tasks_per_second > 100, f"Too slow: {tasks_per_second:.1f} tasks/sec"

        print(f"\nThroughput: {tasks_per_second:.1f} tasks/sec")
        print(f"Mean latency: {result.stats.mean*1000:.1f}ms")

    def test_latency_distribution(self):
        """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œãƒ¬ã‚¤ãƒ†ãƒ³ã‚·åˆ†å¸ƒã‚’æ¸¬å®š."""

        @task
        def single_task() -> str:
            return "done"

        latencies = []
        for i in range(100):
            with workflow(f"latency_test_{i}") as wf:
                wf.add_task(single_task)

                start = time.perf_counter()
                wf.execute()
                latencies.append(time.perf_counter() - start)

        latencies.sort()
        p50 = median(latencies)
        p95 = quantiles(latencies, n=20)[18]  # 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
        p99 = quantiles(latencies, n=100)[98]  # 99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«

        print(f"\nLatency Distribution:")
        print(f"  P50: {p50*1000:.1f}ms")
        print(f"  P95: {p95*1000:.1f}ms")
        print(f"  P99: {p99*1000:.1f}ms")

        # SLAç›®æ¨™ã‚’ã‚¢ã‚µãƒ¼ãƒˆ
        assert p50 < 0.010, f"P50 latency too high: {p50*1000:.1f}ms"
        assert p95 < 0.050, f"P95 latency too high: {p95*1000:.1f}ms"
        assert p99 < 0.100, f"P99 latency too high: {p99*1000:.1f}ms"

    def test_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®š."""
        import tracemalloc

        @task
        def memory_task(i: int) -> List[int]:
            # å°‘ã—ãƒ¡ãƒ¢ãƒªã‚’ä½¿ã†
            return list(range(i * 100))

        tracemalloc.start()

        with workflow("memory_test") as wf:
            tasks = [memory_task.clone(f"task-{i}") for i in range(100)]
            parallel = ParallelGroup(tasks)
            wf.add_task(parallel)
            wf.execute()

        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        print(f"\nMemory Usage:")
        print(f"  Current: {current / 1024 / 1024:.1f} MB")
        print(f"  Peak: {peak / 1024 / 1024:.1f} MB")

        # ãƒ¡ãƒ¢ãƒªç›®æ¨™ã‚’ã‚¢ã‚µãƒ¼ãƒˆ
        assert peak / 1024 / 1024 < 100, f"Peak memory too high: {peak / 1024 / 1024:.1f} MB"
```

**CI/CDã§ã®å®Ÿè¡Œ:**

```bash
# .github/workflows/performance.yml

name: Performance Benchmarks

on:
  pull_request:
  schedule:
    - cron: '0 0 * * 0'  # æ¯é€±æ—¥æ›œæ—¥

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run benchmarks
        run: |
          uvx pytest tests/performance/ --benchmark-only --benchmark-autosave
      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: .benchmarks/benchmark.json
```

**ç›®æ¨™:** ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ç¢ºç«‹

**å·¥æ•°:** 1é€±é–“
**å„ªå…ˆåº¦:** ä¸­ - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã®è¿½è·¡

---

#### 11. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸€è²«æ€§æ”¹å–„

**ç¾çŠ¶:** ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯å­˜åœ¨ã™ã‚‹ãŒä¸€è²«æ€§ã«æ¬ ã‘ã‚‹

**å¯¾ç­–:**

**1) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®docstringã‚’å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ :**

```python
# graflow/hitl/manager.py

"""Human-in-the-Loop ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç®¡ç†.

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œä¸­ã®äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ã®ãŸã‚ã®FeedbackManagerã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
å¯¾å¿œæ©Ÿèƒ½:

- è¤‡æ•°ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¿ã‚¤ãƒ—ï¼ˆæ‰¿èªã€ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã€é¸æŠãªã©ï¼‰
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆçµ±åˆã«ã‚ˆã‚‹ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†
- ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ï¼ˆSlackã€webhookã€ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ï¼‰
- RedisçµŒç”±ã®åˆ†æ•£ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ°¸ç¶šåŒ–

Example:
    >>> from graflow.hitl.manager import FeedbackManager
    >>> manager = FeedbackManager(backend="redis")
    >>> manager.request_feedback(...)

See Also:
    - :mod:`graflow.hitl.types`: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å‹å®šç¾©
    - :mod:`graflow.hitl.notification`: é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
"""
```

**2) Architecture Decision Records (ADR)ã®è¿½åŠ :**

```markdown
# docs/adr/0001-redis-distributed-coordination.md

# Redisä½¿ç”¨ã«ã‚ˆã‚‹åˆ†æ•£ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚·ãƒ§ãƒ³

æ—¥ä»˜: 2025-01-15

## ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹

æ‰¿èªæ¸ˆã¿

## ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œã®ãƒ¯ãƒ¼ã‚«ãƒ¼é–“åˆ†æ•£ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã€‚

## æ±ºå®š

ãƒãƒªã‚¢ã¨pub/subã‚’ä½¿ç”¨ã—ãŸRedisãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¡ç”¨ã€‚

## çµæœ

**ãƒã‚¸ãƒ†ã‚£ãƒ–:**
- å®Ÿç¸¾ã®ã‚ã‚‹ä¿¡é ¼æ€§ã®é«˜ã„æŠ€è¡“
- ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®pub/sub
- çµ„ã¿è¾¼ã¿ã®æ°¸ç¶šåŒ–

**ãƒã‚¬ãƒ†ã‚£ãƒ–:**
- å˜ä¸€éšœå®³ç‚¹ï¼ˆRedis Clusterã§ç·©å’Œï¼‰
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾å­˜
- ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸

## å®Ÿè£…ãƒãƒ¼ãƒˆ

- ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã«ã‚ˆã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ç›£è¦–ã‚’è¿½åŠ ï¼ˆ2025-12-08ï¼‰
- KEYSã‹ã‚‰SCANã¸ã®ç§»è¡Œï¼ˆ2025-12-08ï¼‰
```

**å·¥æ•°:** 2é€±é–“
**å„ªå…ˆåº¦:** ä¸­ - é–‹ç™ºè€…ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹å‘ä¸Š

---

### ğŸŸ¢ ä½å„ªå…ˆï¼ˆLow Priorityï¼‰

#### 12. ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ

**æ¨å¥¨:** Hypothesisã‚’ä½¿ç”¨ã—ãŸã‚³ã‚¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ 

```python
# tests/property/test_graph_properties.py

from hypothesis import given, strategies as st
from graflow.core.graph import TaskGraph

@given(st.lists(st.text(min_size=1, max_size=20), min_size=0, max_size=100))
def test_topological_sort_preserves_all_nodes(task_ids):
    """ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆã¯å…¨ãƒãƒ¼ãƒ‰ã‚’æ­£ç¢ºã«1å›å«ã‚€."""
    # é‡è¤‡ã‚’é™¤å»
    task_ids = list(set(task_ids))

    graph = TaskGraph()
    for task_id in task_ids:
        graph.add_node(create_dummy_task(task_id), task_id)

    sorted_ids = graph.topological_sort()
    assert set(sorted_ids) == set(task_ids)
    assert len(sorted_ids) == len(task_ids)

@given(st.lists(st.tuples(st.text(min_size=1), st.text(min_size=1)), min_size=1))
def test_cycle_detection_is_deterministic(edges):
    """ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºã¯æ±ºå®šçš„ã§ã‚ã‚‹."""
    graph = TaskGraph()
    for src, dst in edges:
        graph.add_edge(src, dst)

    has_cycle1 = graph.has_cycle()
    has_cycle2 = graph.has_cycle()
    assert has_cycle1 == has_cycle2
```

**å·¥æ•°:** 2é€±é–“
**å„ªå…ˆåº¦:** ä½ - å …ç‰¢æ€§å‘ä¸Š

---

#### 13. åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¼æ’­

**æ¨å¥¨:** åˆ†æ•£ã‚·ãƒŠãƒªã‚ªã§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒé©åˆ‡ã«ä¼æ’­ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºä¿

```python
# graflow/trace/propagation.py ï¼ˆæ–°è¦ä½œæˆï¼‰

from typing import Dict, Optional
import uuid

class TraceContext:
    """åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ç”¨ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ."""

    def __init__(
        self,
        trace_id: str,
        span_id: str,
        parent_span_id: Optional[str] = None
    ):
        self.trace_id = trace_id
        self.span_id = span_id
        self.parent_span_id = parent_span_id

    def to_headers(self) -> Dict[str, str]:
        """HTTPãƒ˜ãƒƒãƒ€ãƒ¼ã«å¤‰æ›ï¼ˆä¼æ’­ç”¨ï¼‰."""
        headers = {
            "X-Trace-Id": self.trace_id,
            "X-Span-Id": self.span_id,
        }
        if self.parent_span_id:
            headers["X-Parent-Span-Id"] = self.parent_span_id
        return headers

    @classmethod
    def from_headers(cls, headers: Dict[str, str]) -> "TraceContext":
        """ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º."""
        return cls(
            trace_id=headers.get("X-Trace-Id", str(uuid.uuid4())),
            span_id=headers.get("X-Span-Id", str(uuid.uuid4())),
            parent_span_id=headers.get("X-Parent-Span-Id")
        )
```

**å·¥æ•°:** 1é€±é–“
**å„ªå…ˆåº¦:** ä½ - ç¾åœ¨ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã¯å‹•ä½œã—ã¦ã„ã‚‹ï¼ˆæ‹¡å¼µæ©Ÿèƒ½ï¼‰

---

#### 14. AsyncIOå¯¾å¿œã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

**å•é¡Œ:**
- `wait_barrier`ã®`time.sleep`ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ãŒã‚¨ãƒ³ã‚¸ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åœæ­¢
- ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ã‚„ã‚·ã‚°ãƒŠãƒ«å‡¦ç†ã‚’ä¸¦è¡Œå®Ÿè¡Œã§ããªã„

**æ¨å¥¨:** å°†æ¥ã®ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã«`async def execute(...)`ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®šç¾©ã€ã¾ãŸã¯éãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°`select`/`poll`ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½¿ç”¨

```python
# graflow/core/engine_async.py ï¼ˆå°†æ¥ã®æ‹¡å¼µï¼‰

import asyncio
from typing import Optional, Any

class AsyncWorkflowEngine:
    """éåŒæœŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆå°†æ¥ã®å®Ÿè£…ï¼‰."""

    async def execute(
        self,
        context: ExecutionContext,
        start_task_id: Optional[str] = None
    ) -> Any:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’éåŒæœŸå®Ÿè¡Œ."""
        # ... async/await ãƒ™ãƒ¼ã‚¹ã®å®Ÿè£… ...
```

**å·¥æ•°:** 1é€±é–“ï¼ˆåºƒç¯„ãªãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ï¼‰
**å„ªå…ˆåº¦:** ä½ - å°†æ¥ã®é«˜ä¸¦è¡Œæ€§å¯¾å¿œ

---

## å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### ãƒ•ã‚§ãƒ¼ã‚º1: ä¿¡é ¼æ€§ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆ2-3é€±é–“ï¼‰

**Week 1-2: ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã¨Rediså¯¾å¿œ**
- HITLã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®ãƒˆãƒ¬ãƒ¼ã‚¹çµ‚äº†ãƒ‘ãƒƒãƒ
- Redis KEYS â†’ SCANç½®ãæ›ãˆ
- ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆå®Ÿè£…
- åˆ†æ•£ã‚­ãƒ¥ãƒ¼DLQå®Ÿè£…

**Week 3: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**
- åºƒç¯„ãªä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©ã‚’ç‰¹å®šã®ä¾‹å¤–ã«ç½®ãæ›ãˆ
- æ§‹é€ åŒ–ãƒ­ã‚°è¿½åŠ 
- å¤±æ•—ä¼æ’­ã®ãƒ†ã‚¹ãƒˆè¿½åŠ 

### ãƒ•ã‚§ãƒ¼ã‚º2: ãƒ†ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆ2é€±é–“ï¼‰

**Week 5: çµ±åˆãƒ†ã‚¹ãƒˆ**
- Redis HITLçµ±åˆãƒ†ã‚¹ãƒˆ
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ/ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ãƒ†ã‚¹ãƒˆ
- Langfuseãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ

**Week 6: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**
- pytest-benchmark ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šä½œæˆ
- CIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«è¿½åŠ 

### ãƒ•ã‚§ãƒ¼ã‚º3: ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆ2é€±é–“ï¼‰

**Week 7: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°**
- ExecutionContextã‹ã‚‰å°‚é–€ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’æŠ½å‡º
- ã‚°ãƒ©ãƒ•ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–ï¼ˆCASï¼‰
- LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå›å¾©åŠ›å¼·åŒ–

**Week 8: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**
- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«docstringè¿½åŠ 
- ADRãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
- APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹æ›´æ–°

### ãƒ•ã‚§ãƒ¼ã‚º4: ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«æ‹¡å¼µï¼ˆç¶™ç¶šçš„ï¼‰

- ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
- åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°æ”¹å–„
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

---

## æˆåŠŸæŒ‡æ¨™

| æŒ‡æ¨™ | ç¾çŠ¶ | ç›®æ¨™ | ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ |
|------|------|------|--------------|
| åºƒç¯„ãªä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ© | 6-73 | <10ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ã¿ï¼‰ | 1é€±é–“ |
| Redis KEYSä½¿ç”¨ç®‡æ‰€ | 3 | 0ï¼ˆå…¨ã¦SCANï¼‰ | 1é€±é–“ |
| ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ãƒªã‚«ãƒãƒªæ™‚é–“ | 30ç§’ | <10ç§’ï¼ˆãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆï¼‰ | 3æ—¥ |
| DLQå¯è¦–æ€§ | ãªã— | DLQ+ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼+ãƒ†ã‚¹ãƒˆ | 3æ—¥ |
| HITLã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®ãƒˆãƒ¬ãƒ¼ã‚¹å®Œäº† | ä¸è¶³ | 100%ï¼ˆãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¼ï¼‰ | 1æ—¥ |
| çµ±åˆãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ | é™å®šçš„ | åŒ…æ‹¬çš„ | 2é€±é–“ |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | ãªã— | ç¢ºç«‹æ¸ˆã¿ | 1é€±é–“ |
| ExecutionContext LOC | ~1400 | <800 | 1é€±é–“ |
| ã‚°ãƒ©ãƒ•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç‡ | 100% | <5%ï¼ˆå¤‰æ›´æ™‚ã®ã¿ï¼‰ | 2æ—¥ |

---

## ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

- **å››åŠæœŸã”ã¨ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼**
- **ä¸»è¦æ©Ÿèƒ½è¿½åŠ å¾Œã«æ›´æ–°**
- **GitHub issuesã§é€²æ—è¿½è·¡**
- **CONTRIBUTING.mdã‹ã‚‰ãƒªãƒ³ã‚¯**

---

**æ–‡æ›¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³:** 3.0ï¼ˆçµ±åˆç‰ˆï¼‰
**æœ€çµ‚æ›´æ–°:** 2025-12-08
**æ¬¡å›ãƒ¬ãƒ“ãƒ¥ãƒ¼:** 2026-03-08
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** ã‚¢ã‚¯ãƒ†ã‚£ãƒ–

---

## ä»˜éŒ²: ä¸»è¦ãªé•ã„ã¨çµ±åˆãƒãƒ¼ãƒˆ

**Claudeç‰ˆã¨ã®é•ã„:**
- HITLã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®ãƒˆãƒ¬ãƒ¼ã‚¹çµ‚äº†ã‚’è¿½åŠ ï¼ˆCodexææ¡ˆï¼‰
- Redisã®æœ¬ç•ªå¯¾å¿œã‚’è©³ç´°åŒ–ï¼ˆCodexææ¡ˆï¼‰
- ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚’è¿½åŠ ï¼ˆGeminiææ¡ˆï¼‰
- ã‚°ãƒ©ãƒ•ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–ã‚’è¿½åŠ ï¼ˆGeminiææ¡ˆï¼‰

**Codexç‰ˆã¨ã®é•ã„:**
- ExecutionContextåˆ†è§£ã‚’ä¸­å„ªå…ˆã«å«ã‚ã‚‹ï¼ˆå…¨ã¦å…±é€šï¼‰
- ã‚ˆã‚Šå®Ÿè·µçš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ã‚’æä¾›
- æ—¥æœ¬èªã«ã‚ˆã‚‹è©³ç´°ãªèª¬æ˜

**Geminiç‰ˆã¨ã®é•ã„:**
- AsyncIOå¯¾å¿œã‚’ä½å„ªå…ˆã«å«ã‚ã‚‹
- Schedulerã®æŠ½å‡ºã‚’ä¸­å„ªå…ˆã«å«ã‚ã‚‹
- ã‚ˆã‚Šæ®µéšçš„ãªå®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

**çµ±åˆã®åˆ©ç‚¹:**
- 3ã¤ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰æœ€ã‚‚é‡è¦ãªææ¡ˆã‚’çµ±åˆ
- å®Ÿè£…å„ªå…ˆåº¦ã‚’æ˜ç¢ºåŒ–
- æ—¥æœ¬èªã«ã‚ˆã‚‹åŒ…æ‹¬çš„ãªèª¬æ˜
